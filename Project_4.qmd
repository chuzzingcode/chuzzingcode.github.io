---
title: "Weaponized Data in the AFST"
author: "Chris Hussey"
format:
  html:
    code-fold: true
execute:
  echo: false
  warning: false
  message: false
---

### *The Allegheny Family Screening Tool and the Ethics of Predictive Welfare Surveillance*

What Is the AFST and Why It Matters

The Allegheny Family Screening Tool (AFST) is an algorithm used by Allegheny County, Pennsylvania to help child welfare workers decide whether to investigate a hotline call about child neglect. When a report is filed, the system automatically pulls information from multiple public databases—welfare benefits, criminal justice involvement, juvenile probation, mental health records, and public school data—and uses that information to generate a risk score from 1 to 20, meant to predict the likelihood that a child will be removed from the home within two years.

According to the *Pulitzer Center* (Ho & Burke, 2022), the AFST was developed to help caseworkers manage high call volumes, reduce human bias, and allocate agency resources efficiently. The system is often described as a neutral, objective supplemental tool designed to assist human decision-making. However, as Ballantyne (2023) argues, the algorithm operates in a high-stakes context—its predictions can shape whether families experience intrusive investigations, heightened state surveillance, or even the possibility of temporary or permanent child removal. This combination of administrative data, predictive modeling, and discretionary state power makes the AFST a powerful example of the data science ethical dilemmas highlighted in *Data Feminism*, including the central questions: Who benefits? Who is harmed? and Whose priorities are embedded in the system?

In what follows, I examine four core ethical issues—consent, representativeness, unintended data use, and racial proxies—and analyze how each plays out within this human-algorithm collaboration between social workers and algorithmic predictions.

#1 Consent Structure

The AFST raises significant concerns about whether meaningful consent is possible. Families whose information populates the system never knowingly agreed to have their welfare, education, or criminal justice records combined into an algorithmic decision-making tool. The *Pulitzer Center* article emphasizes that most families do not even know the algorithm exists, much less that it influences whether a caseworker opens an investigation into their home (Ho & Burke, 2022). This means that, despite the deeply personal nature of the data involved, individuals cannot control how their information is used, cannot opt out, and cannot reasonably understand how ordinary interactions with public systems—such as applying for food stamps or receiving school attendance letters—may later be treated as risk indicators.

Ballantyne (2023) further notes that consent is complicated by the inherent power imbalance between families and the state agencies that collect their data. Families interacting with welfare, education, or criminal justice systems typically have no choice but to provide information as a condition of receiving services or complying with legal requirements. This is not voluntary participation in research—it is coerced disclosure embedded in structural inequality. Therefore, informed consent is not only absent; the very possibility of informed consent is undermined by the conditions under which the data are collected.

#2: Who Was Measured / Representativeness

A core issue is who appears in the dataset. The AFST trains exclusively on data from families who interact with public systems, meaning poor and disproportionately Black families are the ones most heavily monitored. The *Pulitzer Center* makes clear that the algorithm draws from welfare records, school data, public mental health services, and criminal justice involvement—systems that are not evenly distributed across socioeconomic lines (Ho & Burke, 2022). Families with higher income who use private healthcare, private schools, or do not rely on public benefits leave far fewer digital traces and therefore remain largely invisible to the model.

This creates a severe representativeness problem. The algorithm purports to assess risk for children across the county, but in practice, it can only “see” those who are already hyper-surveilled through state systems. As Ballantyne (2023) points out, the model does not measure underlying risk; it measures exposure to public institutions. This means the AFST may conflate poverty with danger, treating structural disadvantage as evidence of impending neglect. From the perspective of *Data Feminism*, this demonstrates how power operates—the groups already monitored by the state become the objects of even greater scrutiny.

#3: Is the Data Being Used in Unintended Ways?

The AFST’s use of administrative data illustrates a classic case of function creep, where information collected for one purpose is repurposed for a different, often more punitive, goal. Welfare records were originally gathered to distribute benefits, school records to track educational progress, and criminal justice data to document legal processes. But according to the *Pulitzer Center*, this disparate information is now merged to generate family risk scores (Ho & Burke, 2022). The algorithm reframes routine or structural conditions—such as unemployment, food insecurity, or living in a heavily policed neighborhood—as potential indicators of future child maltreatment.

Ballantyne (2023) argues that this repurposing fundamentally changes the meaning of the data: information that once signaled a need for support now becomes evidence the state could act upon. This raises ethical concerns about whether individuals should be subject to predictive analysis based on data they provided for unrelated administrative reasons. The recontextualization of data, without public awareness or consent, expands the reach of state surveillance under the guise of efficiency.

#4: Should Race Be Used? Is It a Proxy?

Even though race is not explicitly included as a variable, the AFST incorporates numerous proxy variables that reflect racial disparities. Variables such as zip code, welfare usage, school suspensions, juvenile probation, and arrest records are deeply correlated with race due to long-standing structural inequities. The *Pulitzer Center* article notes that these patterns cause families of color—especially Black families—to receive disproportionately higher risk scores (Ho & Burke, 2022). Removing the race variable does not remove racial influence; it only hides it behind factors shaped by residential segregation, policing practices, and unequal access to resources.

Ballantyne (2023) points out that the algorithm’s design reinforces existing inequities by reproducing the racial patterns embedded in the data. Instead of addressing the conditions that shape the data (poverty, discrimination, over-policing), the tool treats these structural forces as individual-level risk factors. This mirrors a key question from the *Data Feminism* framework: Is the issue really “bias in the algorithm,” or is the algorithm simply reflecting unjust systems that already exist?

Why This Matters:

The AFST illustrates how data science systems can shift power toward institutions and away from the people they govern. Who benefits? The county and its caseworkers gain efficiency and predictive tools that help allocate scarce resources. Who is harmed? Families who are already marginalized face heightened surveillance, invasive investigations, and greater risk of family separation. As Ballantyne (2023) emphasizes, even the best-intentioned human–AI collaborations can cause harm when they reinforce unequal power structures. And as the *Pulitzer Center* shows, the ethical dilemmas here are not hypothetical—the algorithm’s risk scores shape real decisions with life-altering consequences.

Ultimately, this case reflects the broader pattern described in *Data Feminism*: data science is often used “in the service of surveillance (of the minoritized) and efficiency (amidst scarcity).” The AFST is not just a technical tool; it is a form of governance that makes certain families more visible to the state while others remain unseen. It embeds social inequalities into mathematical form, presenting them as objective truth. This matters because data-driven decisions often carry an aura of neutrality that masks the power relations beneath. Understanding these dynamics is crucial if data science is to be used ethically and responsibly in social systems.

CASE 2: NYC, Stop and Frisk, and Predictive Policing.

This analysis explores the distribution of NYPD stop-and-frisk encounters across racial groups using publicly reported 2024 data. This approach highlights who appears in the administrative record, which is essential for understanding how predictive systems—whether in policing or child welfare—learn patterns of state surveillance rather than neutral patterns of risk.

```{r}
library(tidyverse)
library(readxl)

sf <- read_excel("~/Downloads/sqf-2024.xlsx")

sf_clean <- sf |>
  mutate(
    race   = SUSPECT_RACE_DESCRIPTION,
    frisk  = FRISKED_FLAG == "Y",
    search = SEARCHED_FLAG == "Y",
    arrest = SUSPECT_ARRESTED_FLAG == "Y",
    weapon = WEAPON_FOUND_FLAG == "Y"
  ) |>
  filter(!is.na(race), race != "(null)")


```

![]()

```{r}

summary_race <- sf_clean |>
  group_by(race) |>
  summarize(
    stops = n(),
    frisks = sum(frisk,   na.rm = TRUE),
    searches = sum(search,  na.rm = TRUE),
    arrests = sum(arrest,  na.rm = TRUE),
    weapons = sum(weapon,  na.rm = TRUE)
  ) |>
  mutate(
    prop_stops = stops    / sum(stops),
    prop_frisks = frisks   / sum(frisks),
    prop_searches = searches / sum(searches),
    prop_arrests = arrests  / sum(arrests),
    prop_weapons = weapons  / sum(weapons)
  ) |>
  arrange(desc(prop_stops))


```

![]()

Search rate by race:

```{r}
summary_race |>
  ggplot(aes(x = reorder(race, prop_stops), y = prop_stops)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Share of All NYPD Stops by Race — 2024",
    x = "Race",
    y = "Proportion of All Stops",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  )


```

Search Rate by Race

```{r}
summary_race |>
  ggplot(aes(x = reorder(race, prop_searches), y = prop_searches)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Share of All NYPD Searches by Race — 2024",
    x = "Race",
    y = "Proportion of All Searches",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  )


```

Arrest rate by race

```{r}
summary_race |>
  ggplot(aes(x = reorder(race, prop_arrests), y = prop_arrests)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Share of All NYPD Arrests by Race — 2024",
    x = "Race",
    y = "Proportion of All Arrests",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  )

```

```{r}
black_df <- sf_clean |>
  filter(race == "BLACK") |>
  summarize(
    black_stops = n(),
    black_arrests = sum(arrest, na.rm = TRUE)
  ) |>
  mutate(
    black_non_arrests = black_stops - black_arrests
  )

black_plot <- black_df |>
  pivot_longer(
    cols = c(black_arrests, black_non_arrests),
    names_to = "outcome",
    values_to = "count"
  ) |>
  mutate(
    outcome = if_else(outcome == "black_arrests", "Arrests", "Not Arrested")
  )


```

```{r}
black_plot |>
  mutate(
    percent = count / sum(count)
  ) |>
  ggplot(aes(x = "", y = percent, fill = outcome)) +
  geom_col(width = 1) +
  geom_text(aes(label = scales::percent(percent, accuracy = 0.1)),
            position = position_stack(vjust = 0.5)) +
  coord_polar(theta = "y") +
  labs(
    title = "What Share of Black Stops Resulted in Arrests? — NYPD 2024",
    fill = "",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  ) +
  theme_void()

```

Case 1 References

Ballantyne, N. (2023). *The harm that data do: The case of the Allegheny Family Screening Tool*. Medium. [https://medium.com/\@neilballantyne/the-harm-that-data-do-the-case-of-the-allegheny-family-screening-tool-5f9fca22e0b2](https://medium.com/@neilballantyne/the-harm-that-data-do-the-case-of-the-allegheny-family-screening-tool-5f9fca22e0b2)

Ho, S., & Burke, G. (2022). *An algorithm that screens for child neglect raises concerns*. Pulitzer Center. <https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns>

Case 2 References:

Lum, K., & Isaac, W. (2016). *To predict and serve?* **Significance, 13**(5), 14–19. Royal Statistical Society. <https://doi.org/10.1111/j.1740-9713.2016.00960.x> [[Wiley Online Library+1]{.underline}](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x?utm_source=chatgpt.com){alt="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x?utm_source=chatgpt.com"}

Joh, E. E. (2017). *Feeding the machine: Policing, crime data, & algorithms.* **William & Mary Bill of Rights Journal, 26**, 287–329. [https://scholarship.law.wm.edu/wmborj/vol26/iss2/3](https://scholarship.law.wm.edu/wmborj/vol26/iss2/3?utm_source=chatgpt.com) [[W&M Law School Scholarship Repository+2W&M Law School Scholarship Repository+2]{.underline}](https://scholarship.law.wm.edu/wmborj/vol26/iss2/3/?utm_source=chatgpt.com){alt="https://scholarship.law.wm.edu/wmborj/vol26/iss2/3/?utm_source=chatgpt.com"}

Robinson, D., & Koepke, L. (2016). *Stuck in a pattern: Early evidence on “predictive policing” and civil rights.* Upturn. https://www.teamupturn.com/reports/2016/stuck-in-a-pattern [[Upturn+2Upturn+2]{.underline}](https://www.upturn.org/static/reports/2016/stuck-in-a-pattern/files/Upturn_-_Stuck_In_a_Pattern_v.1.01.pdf?utm_source=chatgpt.com){alt="https://www.upturn.org/static/reports/2016/stuck-in-a-pattern/files/Upturn_-_Stuck_In_a_Pattern_v.1.01.pdf?utm_source=chatgpt.com"}

Kofman, A., & Scheiber, N. (2023, October 2). *Predictive policing software terrible at predicting crimes.* **The Markup** (with Northeastern University). [https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes](https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes?utm_source=chatgpt.com) [[The Markup+2College of Social Sciences+2]{.underline}](https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes?utm_source=chatgpt.com){alt="https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes?utm_source=chatgpt.com"}

Leadership Conference on Civil and Human Rights. (2016, August 31). *Civil rights and tech advocates sound alarm on racial bias in “predictive policing.”* [https://civilrights.org/2016/08/31/civil-rights-and-tech-advocates-sound-alarm-on-racial-bias-in-predictive-policing/](https://civilrights.org/2016/08/31/civil-rights-and-tech-advocates-sound-alarm-on-racial-bias-in-predictive-policing/?utm_source=chatgpt.com) [[Leadership Conference+2The Guardian+2]{.underline}](https://civilrights.org/2016/08/31/civil-rights-and-tech-advocates-sound-alarm-on-racial-bias-in-predictive-policing/?utm_source=chatgpt.com){alt="https://civilrights.org/2016/08/31/civil-rights-and-tech-advocates-sound-alarm-on-racial-bias-in-predictive-policing/?utm_source=chatgpt.com"}

Wyden, R., et al. (2024, January 29). *US lawmakers tell DOJ to quit blindly funding “predictive” police tools.* **WIRED.** [https://www.wired.com/story/doj-predictive-policing-lawmakers-demand/](https://www.wired.com/story/doj-predictive-policing-lawmakers-demand/?utm_source=chatgpt.com) [[WIRED]{.underline}](https://www.wired.com/story/doj-predictive-policing-lawmakers-demand?utm_source=chatgpt.com){alt="https://www.wired.com/story/doj-predictive-policing-lawmakers-demand?utm_source=chatgpt.com"}
