---
title: "Ethics In Predictive Policing"
author: "Chris Hussey"
subtitle: "12/10/25"
format:
  revealjs:
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
execute:
  echo: false
  warning: false
  message: false
---

---

## Introduction: AFST & Ethical Questions

-   My original AFST project examined how administrative data, including welfare, schooling, juvenile justice, feeds into a child risk-scoring algorithm.
-   Key ethical issues: consent, representativeness, function creep, racial proxies.
-   I wondered how I could connect this to my thesis topic which looks at our diminished capacity to consent to state coercion enabled by private actors.
-   Today: I extend that analysis using **NYPD stop-and-frisk data** to indicate that a similar structure is operating inside predictive policing.

---

## Administrative Data ≠ Crime Data

NYC Open

-   Stop-and-frisk is not a measure of crime.
-   It is a measure of **policing activity**: who police stop, search, and arrest.
-   This makes it structurally similar to AFST data:
    -   It reflects **visibility to the state**,
    -   not objective risk.
-   Visualizing this data makes the bias visible.

---

## Data Set

```{r}
library(tidyverse)
library(readxl)

sf <- read_excel("~/Downloads/sqf-2024.xlsx")

sf_clean <- sf |>
  mutate(
    race   = SUSPECT_RACE_DESCRIPTION,
    frisk  = FRISKED_FLAG == "Y",
    search = SEARCHED_FLAG == "Y",
    arrest = SUSPECT_ARRESTED_FLAG == "Y",
    weapon = WEAPON_FOUND_FLAG == "Y"
  ) |>
  filter(!is.na(race), race != "(null)")


summary_race <- sf_clean |>
  group_by(race) |>
  summarize(
    stops = n(),
    frisks = sum(frisk,   na.rm = TRUE),
    searches = sum(search,  na.rm = TRUE),
    arrests = sum(arrest,  na.rm = TRUE),
    weapons = sum(weapon,  na.rm = TRUE)
  ) |>
  mutate(
    prop_stops = stops / sum(stops),
    prop_frisks = frisks / sum(frisks),
    prop_searches = searches / sum(searches),
    prop_arrests = arrests / sum(arrests),
    prop_weapons = weapons/ sum(weapons)
  ) |>
  arrange(desc(prop_stops))

head(summary_race)
```

---

## Who Gets Stopped? 

```{r}
library(ggplot2)
summary_race |>
  ggplot(aes(x = reorder(race, prop_stops), y = prop_stops)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Share of All NYPD Stops by Race — 2024",
    x = "Race",
    y = "Proportion of All Stops",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  )

```

---

## Who Gets Arrested?

```{r}
summary_race |>
  ggplot(aes(x = reorder(race, prop_arrests), y = prop_arrests)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Share of All NYPD Arrests by Race — 2024",
    x = "Race",
    y = "Proportion of All Arrests",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  )
```

---

## Within-Group Escalation: Black Stops → Arrests

```{r}
black_df <- sf_clean |>
  filter(race == "BLACK") |>
  summarize(
    black_stops = n(),
    black_arrests = sum(arrest, na.rm = TRUE)
  ) |>
  mutate(
    black_non_arrests = black_stops - black_arrests
  )

black_plot <- black_df |>
  pivot_longer(
    cols = c(black_arrests, black_non_arrests),
    names_to = "outcome",
    values_to = "count"
  ) |>
  mutate(
    outcome = if_else(outcome == "black_arrests", "Arrests", "Not Arrested")
  )

black_plot |>
  mutate(
    percent = count / sum(count)
  ) |>
  ggplot(aes(x = "", y = percent, fill = outcome)) +
  geom_col(width = 1) +
  geom_text(aes(label = scales::percent(percent, accuracy = 0.1)),
            position = position_stack(vjust = 0.5)) +
  coord_polar(theta = "y") +
  labs(
    title = "What Share of Black Stops Resulted in Arrests? — NYPD 2024",
    fill = "",
    caption = "Data: NYPD Stop-and-Frisk 2024"
  ) +
  theme_void()

```

---

## **Predictive Policing in 60 Seconds**

-   Tools like **PredPol/Geolitica** use past incident reports (and sometimes stops/arrests) to forecast “high-risk” locations.

-   But if the data reflects **racially skewed enforcement**, predictions simply reroute police to the same communities.

-   Research shows:

    -   \<1% accuracy in Plainfield, NJ (The Markup)

    -   Reinforced racial patterns in Oakland (Lum & Isaac)

    -   Programs discontinued in LA and Chicago after civil-rights audits.

-   The algorithm learns **policing**, not crime.

---

## **How This Mirrors AFST**

### **Same structural pattern:**

-   **Representativeness:**

    -   AFST sees families in public systems.

    -   Policing algorithms see people whom police choose to stop.

-   **Function Creep:**

    -   AFST repurposes welfare/education records for prediction.

    -   Policing repurposes stop/arrest data for patrol forecasts.

-   **Proxies for Race:**

    -   Neighborhood, school discipline, benefits = racialized proxies.

    -   Stop locations, arrest histories = racialized policing proxies.

-   **Feedback Loop:**

    -   More intervention = more data = more predicted risk = more intervention.

---

## **Ethical Evaluation**

Using Data Feminism’s lens:

-   **Examine Power:** These systems amplify the visibility of groups already heavily monitored.

-   **Challenge Neutrality:** Risk scores and “hotspots” are institutional histories disguised as objective predictions.

-   **Consider Harm:** Algorithmic decisions escalate surveillance and state intervention in marginalized communities.

---

## **Potential Conclusions?** 

Predictive systems built from institutional data seem to more accurately measure **institutional behavior**.
The AFST and NYPD datasets indicate the same structural logic, that data becomes a weapon when it encodes unequal power.

---

Thank You!

---
