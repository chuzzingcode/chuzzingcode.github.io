[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher Hussey",
    "section": "",
    "text": "Hi! My name’s Christopher (though, I usually go by Chris), and welcome to my website! I’m a Politics, Philosophy, and Economics major at Pomona College, with a burgeoning interest in Data Science and all the visualizations that come with it. Some of my academic areas of interest are security, international relations, Post Structuralist philosophy, and surveillance! Outside of class, you can catch me skating, planning events, and stressing about class!"
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "Project_3",
    "section": "",
    "text": "hat you plan to do (3-4 sentences). end with a description of what you did (3-4 sentences). That is, use words to guide the reader through your analysis\nIn this project I will test whether country-level recognition rates for Syrian asylum applicants differ between two small, meaningful blocs, the Big 4 Nations in the EU —Germany, France, Italy, Spain— and Nordic nations—Sweden, Norway, Denmark. I’ll focus on the general peak of the refugee crisis (between 2014-2017, inclusive).\n\n\nShow code\nlibrary(tidyverse)\nlibrary(refugees)\n\n\nThis code creates a data frame that keeps the relevant EU and Nordic countries that received asylum claims from Syria between the years of 2014 and 2024. It selects the COO ISO code for Syria (“SYR”), the COA ISO codes for the EU and Nordic Blocs respectively, then calculates their recognition rates using rec_total, which summarizes the decision(whether it was through official recognition channels through UNHCR’s mandate or other forms of asylum protection, and n_total, which is the total number of applications received by a given country, and dividing rec_total by n_total. It then outputs a tibble with each asylum country, their rate, and their bloc. It also removes NAs and prevents dividing by 0 during the rate calculations. It also returns an table with the mean and median recognition rates aggregated by bloc.\n\n\nShow code\n# focus\norigin_sel &lt;- \"SYR\"\n\nref_slice &lt;- asylum_decisions|&gt;\n  select(year, coo_iso, coa_iso, dec_recognized, dec_other, dec_total)|&gt;\n  filter(coo_iso == origin_sel,\n         year &gt;= 2014,\n         (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\" |\n          coa_iso == \"SWE\" | coa_iso == \"NOR\" | coa_iso == \"DNK\"))|&gt;\n  group_by(coa_iso)|&gt;\n  summarise(\n    rec_total = sum(dec_recognized + dec_other, na.rm = TRUE),\n    n_total   = sum(dec_total, na.rm = TRUE),\n    rec_rate  = if_else(n_total &gt; 0, rec_total / n_total, NA_real_),\n  )|&gt;\n    mutate(\n    bloc = case_when(\n      (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\") ~ \"EU Big-4\",\n      (coa_iso == \"SWE\" | coa_iso == \"NOR\" | coa_iso == \"DNK\")                   ~ \"Nordics\",\n      TRUE ~ NA_character_\n    )\n  )|&gt;\n  filter(!is.na(bloc), !is.na(rec_rate))|&gt;\n  mutate(bloc = factor(bloc, levels = c(\"EU Big-4\",\"Nordics\")))\n\nref_slice\n\n\n# A tibble: 7 × 5\n  coa_iso rec_total n_total rec_rate bloc    \n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   \n1 DEU        918795 1147929    0.800 EU Big-4\n2 DNK         18343   19321    0.949 Nordics \n3 ESP         19225   22570    0.852 EU Big-4\n4 FRA         36636   43479    0.843 EU Big-4\n5 ITA          4382    5442    0.805 EU Big-4\n6 NOR         16831   21407    0.786 Nordics \n7 SWE         95822  117367    0.816 Nordics \n\n\nShow code\nggplot(ref_slice, aes(x = bloc, y = rec_rate)) +\n  geom_boxplot(width = 0.55, outlier.shape = NA) +\n  geom_jitter(width = 0.08, alpha = 0.8) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Country recognition rates (2014–2024)\",\n    subtitle = \"SYR applicants | one point per country; box summarizes each bloc\",\n    x = NULL, y = \"Recognition rate\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nThe below code provides the aggregate visualization to calculate the scores between EU and Nordic companies\n\n\nShow code\nbloc_avgs_unweighted &lt;- ref_slice |&gt;\n  group_by(bloc) |&gt;\n  summarise(\n    mean_rate   = mean(rec_rate),\n    median_rate = median(rec_rate),\n    n_countries = n(),\n    .groups = \"drop\"\n  )\n\nbloc_avgs_unweighted\n\n\n# A tibble: 2 × 4\n  bloc     mean_rate median_rate n_countries\n  &lt;fct&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;int&gt;\n1 EU Big-4     0.825       0.824           4\n2 Nordics      0.851       0.816           3\n\n\nI used the below function to randomly assign data points to each\n\n\nShow code\n  perm_data &lt;- function(rep, data){\n  data |&gt; \n    select(bloc, rec_rate) |&gt; \n    mutate(rec_rate_perm = sample(rec_rate, replace = FALSE)) |&gt; \n    group_by(bloc) |&gt; \n    summarise(obs_ave = mean(rec_rate),      \n              obs_med = median(rec_rate),\n              perm_ave = mean(rec_rate_perm),\n              perm_med = median(rec_rate_perm)) |&gt;\n    summarise(obs_ave_diff = diff(obs_ave),\n              obs_med_diff = diff(obs_med),\n              perm_ave_diff = diff(perm_ave),\n              perm_med_diff = diff(perm_med),\n              rep = rep)\n  }\n\n\nI will generate 500 simulated null data sets to show the distribution\n\n\nShow code\nset.seed(47)\n\nperm_stats &lt;- map(c(1:5000), perm_data, data = ref_slice) |&gt; \n  list_rbind()\n\n\n# --- null histograms with observed vertical lines ---\nperm_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nShow code\nperm_stats |&gt; \n  ggplot(aes(x = perm_med_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_med_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow code\nperm_summary &lt;- perm_stats %&gt;% \n  summarise(\n    p_val_mean_two_sided   = mean(abs(perm_ave_diff)   &gt;= abs(obs_ave_diff)),\n    p_val_median_two_sided = mean(abs(perm_med_diff)   &gt;= abs(obs_med_diff)),\n    obs_mean_diff          = first(obs_ave_diff),\n    obs_median_diff        = dplyr::first(obs_med_diff)\n  )\n\nperm_summary\n\n\n# A tibble: 1 × 4\n  p_val_mean_two_sided p_val_median_two_sided obs_mean_diff obs_median_diff\n                 &lt;dbl&gt;                  &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1                0.740                  0.942        0.0257        -0.00749"
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Patterns in Netflix Shows",
    "section": "",
    "text": "This page looks at the TidyTuesday Netflix Titles dataset from 2021-04-20, which includes metadata and short descriptions for movies and TV shows. In my plots, I compare movie runtimes by decade and title lengths across shows in the data set.\n\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\n\n\n\nnetflix &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv\",\n)\n\nclean_data &lt;- netflix |&gt;\n  select(type, title, date_added, release_year, duration, description) |&gt;\n  mutate(\n    title       = str_squish(title),\n    description = str_squish(description),\n    decade      = (release_year %/% 10) * 10\n  )\n\nparsed_data &lt;- clean_data |&gt;\n  mutate(\n    minutes = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*min$)\")),\n    seasons = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*Season(?:s)?$)\"))\n  )\n\n\n\n\nShow code\nparsed_data |&gt;\n  filter(type == \"Movie\", !is.na(minutes), !is.na(decade)) |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  ggplot(aes(x = minutes, fill = decade)) +\n  geom_density(alpha = 0.25) +\n  labs(\n    title = \"Movie runtime distributions by decade\",\n    x = \"Runtime (minutes)\",\n    y = \"Density\",\n    fill = \"Decade\"\n  )\n\n\n\n\n\n\n\n\n\nEach curve on this plot shows the distribution of Netflix movie runtimes for each decade represented since the 40s. The higher peaks show the most common lengths for that decade. What we see in this plot are the changes across time for what a typical runtime looks like in each era, with the peaks showing what the most frequent lengths were for a given decade.\n\n\nShow code\nparsed_data |&gt;\n  transmute(\n    title_words = title |&gt;\n      str_squish() |&gt;\n      str_count(\"\\\\S+\")         \n  ) |&gt;\n  filter(!is.na(title_words)) |&gt;\n  ggplot(aes(x = title_words)) +\n  geom_histogram(binwidth = 1, center = 0.5) +\n  labs(\n    title = \"How long are Netflix titles?\",\n    subtitle = \"Histogram of the number of words per title\",\n    x = \"Words in title\",\n    y = \"Number of titles\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar on this plot shows how many Netflix titles have a given number of words in their names. Taller bars mark the most common title lengths. What we see is that short, punchy names (around 1–3 words) are most frequent, with a tapering tail of longer titles that tend to appear less often.\n\n\nShow the code\nparsed_data |&gt;\n  filter(type == \"TV Show\", !is.na(seasons), seasons &lt;= 11) |&gt;\n  count(seasons, sort = FALSE) |&gt;\n  mutate(seasons = factor(seasons, levels = sort(unique(seasons)))) |&gt;\n  ggplot(aes(x = seasons, y = n)) +\n  geom_col() +\n  labs(\n    title = \"How many seasons do Netflix TV shows have?\",\n    subtitle = \"Counts of TV shows by stated season count (shows with &gt;11 seasons excluded)\",\n    x = \"Seasons\",\n    y = \"Number of TV shows\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar shows how many Netflix TV shows report a given number of seasons, 1 - 11. The tallest bars at the low season counts show that most Netflix series in the catalog are shorter/only have 1 season. This plot excluded series with more than 11 seasons for readability.\nReferences:\n\nTidyTuesday – Netflix Titles (2021-04-20)\nrfordatascience/tidytuesday (CSV + README).\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-04-20\nOriginal Dataset – Netflix Movies and TV Shows (Kaggle)\nCommunity-compiled catalog commonly cited as the underlying source.\nhttps://www.kaggle.com/datasets/shivamb/netflix-shows"
  },
  {
    "objectID": "tidydata2.html",
    "href": "tidydata2.html",
    "title": "My Second Tidy Visualization",
    "section": "",
    "text": "# Clean data provided by Kat Correia. Student interns extracted this information\n# from reproductive medicine journals using duplicate data entry. \n# After duplicate data entry discrepancies were resolved, their \n# individual files were combined, and posted for public access \n# as Google sheets, e.g.,:\n# https://docs.google.com/spreadsheets/d/1UuWGQxRU0wxVuOZGYnvkwKn-GdmLIj8kzmUm4KLRHQY/edit?gid=1719021104#gid=1719021104\n# No additional cleaning was necessary.\n\narticle_dat &lt;- readr::read_csv(\"https://kcorreia.people.amherst.edu/repro_med_disparities-article-level-data.csv\")\n\nRows: 318 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (26): doi, jabbrv, journal, month, day, title, abstract, keywords, study...\ndbl (35): pmid, year, study_year_start, study_year_end, race1_ss, race2_ss, ...\nlgl  (4): eth7, eth7_ss, eth8, eth8_ss\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmodel_dat &lt;- readr::read_csv(\"https://kcorreia.people.amherst.edu/repro_med_disparities-model-level-data.csv\")\n\nRows: 6804 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): doi, stratified, stratgrp, subanalysis, subgrp, outcome, measure, ...\ndbl  (5): model_number, comparison, point, lower, upper\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "tidydata1.html",
    "href": "tidydata1.html",
    "title": "Lack of Complete Indoor Plumbing: 2023 vs 2022",
    "section": "",
    "text": "In this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nTidy Tuesday Data\nCensus Data: U.S. Census Bureau. American Community Survey (ACS) 1-year estimates, Tables B01003 & B25049 (2022–2023)."
  },
  {
    "objectID": "tidydata1.html#this-data-set-came-from-the-u.s.-census-bureaus-american-community-survey-acs-for-2022-and-2023.-it-tracks-county-level-population-and-the-share-of-households-without-complete-plumbing-letting-us-see-how-access-to-basic-facilities-has-changed-over-time.",
    "href": "tidydata1.html#this-data-set-came-from-the-u.s.-census-bureaus-american-community-survey-acs-for-2022-and-2023.-it-tracks-county-level-population-and-the-share-of-households-without-complete-plumbing-letting-us-see-how-access-to-basic-facilities-has-changed-over-time.",
    "title": "Lack of Complete Indoor Plumbing: 2023 vs 2022",
    "section": "",
    "text": "In this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nTidy Tuesday Data\nCensus Data: U.S. Census Bureau. American Community Survey (ACS) 1-year estimates, Tables B01003 & B25049 (2022–2023)."
  }
]