[
  {
    "objectID": "final_pres.html#introduction-afst-ethical-questions",
    "href": "final_pres.html#introduction-afst-ethical-questions",
    "title": "Ethics In Predictive Policing",
    "section": "Introduction: AFST & Ethical Questions",
    "text": "Introduction: AFST & Ethical Questions\n\nMy original AFST project examined how administrative data, including welfare, schooling, juvenile justice, feeds into a child risk-scoring algorithm.\nKey ethical issues: consent, representativeness, function creep, racial proxies.\nI wondered how I could connect this to my thesis topic which looks at our diminished capacity to consent to state coercion enabled by private actors.\nToday: I extend that analysis using NYPD stop-and-frisk data to indicate that a similar structure is operating inside predictive policing."
  },
  {
    "objectID": "final_pres.html#administrative-data-crime-data",
    "href": "final_pres.html#administrative-data-crime-data",
    "title": "Ethics In Predictive Policing",
    "section": "Administrative Data ≠ Crime Data",
    "text": "Administrative Data ≠ Crime Data\nNYC Open\n\nStop-and-frisk is not a measure of crime.\nIt is a measure of policing activity: who police stop, search, and arrest.\nThis makes it structurally similar to AFST data:\n\nIt reflects visibility to the state,\nnot objective risk.\n\nVisualizing this data makes the bias visible."
  },
  {
    "objectID": "final_pres.html#data-set",
    "href": "final_pres.html#data-set",
    "title": "Ethics In Predictive Policing",
    "section": "Data Set",
    "text": "Data Set\n\n\n# A tibble: 6 × 11\n  race              stops frisks searches arrests weapons prop_stops prop_frisks\n  &lt;chr&gt;             &lt;int&gt;  &lt;int&gt;    &lt;int&gt;   &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 BLACK             14662   9584     5974    3955    2155    0.601       0.646  \n2 WHITE HISPANIC     5177   2915     2067    1482     679    0.212       0.197  \n3 BLACK HISPANIC     2485   1555      940     634     377    0.102       0.105  \n4 WHITE              1367    466      606     531     116    0.0560      0.0314 \n5 ASIAN / PACIFIC …   435    184      213     172      44    0.0178      0.0124 \n6 MIDDLE EASTERN/S…   228    106       82      67      24    0.00935     0.00715\n# ℹ 3 more variables: prop_searches &lt;dbl&gt;, prop_arrests &lt;dbl&gt;,\n#   prop_weapons &lt;dbl&gt;"
  },
  {
    "objectID": "final_pres.html#who-gets-stopped",
    "href": "final_pres.html#who-gets-stopped",
    "title": "Ethics In Predictive Policing",
    "section": "Who Gets Stopped?",
    "text": "Who Gets Stopped?"
  },
  {
    "objectID": "final_pres.html#who-gets-arrested",
    "href": "final_pres.html#who-gets-arrested",
    "title": "Ethics In Predictive Policing",
    "section": "Who Gets Arrested?",
    "text": "Who Gets Arrested?"
  },
  {
    "objectID": "final_pres.html#within-group-escalation-black-stops-arrests",
    "href": "final_pres.html#within-group-escalation-black-stops-arrests",
    "title": "Ethics In Predictive Policing",
    "section": "Within-Group Escalation: Black Stops → Arrests",
    "text": "Within-Group Escalation: Black Stops → Arrests"
  },
  {
    "objectID": "final_pres.html#predictive-policing-in-60-seconds",
    "href": "final_pres.html#predictive-policing-in-60-seconds",
    "title": "Ethics In Predictive Policing",
    "section": "Predictive Policing in 60 Seconds",
    "text": "Predictive Policing in 60 Seconds\n\nTools like PredPol/Geolitica use past incident reports (and sometimes stops/arrests) to forecast “high-risk” locations.\nBut if the data reflects racially skewed enforcement, predictions simply reroute police to the same communities.\nResearch shows:\n\n&lt;1% accuracy in Plainfield, NJ (The Markup)\nReinforced racial patterns in Oakland (Lum & Isaac)\nPrograms discontinued in LA and Chicago after civil-rights audits.\n\nThe algorithm learns policing, not crime."
  },
  {
    "objectID": "final_pres.html#how-this-mirrors-afst",
    "href": "final_pres.html#how-this-mirrors-afst",
    "title": "Ethics In Predictive Policing",
    "section": "How This Mirrors AFST",
    "text": "How This Mirrors AFST\nSame structural pattern:\n\nRepresentativeness:\n\nAFST sees families in public systems.\nPolicing algorithms see people whom police choose to stop.\n\nFunction Creep:\n\nAFST repurposes welfare/education records for prediction.\nPolicing repurposes stop/arrest data for patrol forecasts.\n\nProxies for Race:\n\nNeighborhood, school discipline, benefits = racialized proxies.\nStop locations, arrest histories = racialized policing proxies.\n\nFeedback Loop:\n\nMore intervention = more data = more predicted risk = more intervention."
  },
  {
    "objectID": "final_pres.html#ethical-evaluation",
    "href": "final_pres.html#ethical-evaluation",
    "title": "Ethics In Predictive Policing",
    "section": "Ethical Evaluation",
    "text": "Ethical Evaluation\nUsing Data Feminism’s lens:\n\nExamine Power: These systems amplify the visibility of groups already heavily monitored.\nChallenge Neutrality: Risk scores and “hotspots” are institutional histories disguised as objective predictions.\nConsider Harm: Algorithmic decisions escalate surveillance and state intervention in marginalized communities."
  },
  {
    "objectID": "final_pres.html#potential-conclusions",
    "href": "final_pres.html#potential-conclusions",
    "title": "Ethics In Predictive Policing",
    "section": "Potential Conclusions?",
    "text": "Potential Conclusions?\nPredictive systems built from institutional data seem to more accurately measure institutional behavior. The AFST and NYPD data sets indicate the same structural logic, that data becomes a weapon when it encodes unequal power."
  },
  {
    "objectID": "tidydata1.html",
    "href": "tidydata1.html",
    "title": "Changes in Household Plumbing Access, 2022–2023",
    "section": "",
    "text": "This analysis uses county-level data from the U.S. Census Bureau’s American Community Survey (ACS) 1-year estimates for 2022 and 2023. I focus on two variables 1) Total population in each county (Table B01003) and 2) Number of households lacking complete plumbing (Table B25049)\nUsing these, I calculate the percentage of households without complete indoor plumbing and then compare 2022 to 2023 across all U.S. counties included in the ACS 1-year sample. The goal is to understand whether counties with higher plumbing insecurity in 2022 tended to see improvement, worsening, or stability in 2023.\nThis data set came from the U.S. Census Bureau’s American Community Survey (ACS) for 2022 and 2023. It tracks county-level population and the share of households without complete plumbing, letting us see how access to basic facilities has changed over time.\n\n\nCode\n# Clean data compiled from code referenced in article (https://waterdata.usgs.gov/blog/acs-maps/). \n# Code was revised to pull data for all US counties for years 2022 - 2023.\n\n# Load packages -----\nlibrary(tidycensus)\nlibrary(sf) \nlibrary(janitor) \nlibrary(tidyverse)\n\n\n\n# Helper functions -----\nget_census_data &lt;- function(geography, var_names, year, proj, survey_var) {\n  df &lt;- get_acs(\n    geography = geography,\n    variable = var_names,\n    year = year,\n    geometry = FALSE,\n    survey = survey_var) |&gt;\n    clean_names() |&gt;\n    mutate(year = year)\n  \n  return(df) \n}\n\n# Grab relevant variables - B01003_001: total population, B25049_004: households lacking plumbing----\nvars &lt;- c(\"B01003_001\", \"B25049_004\")\n\n# Pull data for 2023 and 2022 for all US counties ------\nwater_insecurity_2023 &lt;- get_census_data(\n  geography = 'county', \n  var_names = vars, \n  year = \"2023\", \n  proj = \"EPSG:5070\", \n  survey_var = \"acs1\"\n) |&gt;\n  mutate(\n    variable_long = case_when(\n      variable == \"B01003_001\" ~ \"total_pop\",\n      variable == \"B25049_004\" ~ \"plumbing\",\n      .default = NA_character_  \n    )\n  ) |&gt; \n  select(geoid, name, variable_long, estimate, year) |&gt; \n  pivot_wider(\n    names_from = variable_long,\n    values_from = estimate\n  ) |&gt; \n  mutate(\n    percent_lacking_plumbing = (plumbing / total_pop) * 100\n  )\n\nwater_insecurity_2022 &lt;- get_census_data(\n  geography = 'county', \n  var_names = vars, \n  year = \"2022\", \n  proj = \"EPSG:5070\", \n  survey_var = \"acs1\"\n) |&gt;\n  mutate(\n    variable_long = case_when(\n      variable == \"B01003_001\" ~ \"total_pop\",\n      variable == \"B25049_004\" ~ \"plumbing\",\n      .default = NA_character_  \n    )\n  ) |&gt; \n  select(geoid, name, variable_long, estimate, year) |&gt; \n  pivot_wider(\n    names_from = variable_long,\n    values_from = estimate\n  ) |&gt; \n  mutate(\n    percent_lacking_plumbing = (plumbing / total_pop) * 100\n  )\n\n\n\n\nCode\n# Combine 2022 and 2023 data into one comparison data frame\nwi23 &lt;- water_insecurity_2023 |&gt; \n  as.data.frame() |&gt; \n  select(geoid, name, percent_lacking_plumbing) |&gt; \n  rename(p23 = percent_lacking_plumbing)\n\nwi22 &lt;- water_insecurity_2022 |&gt; \n  as.data.frame() |&gt; \n  select(geoid, name, percent_lacking_plumbing) |&gt; \n  rename(p22 = percent_lacking_plumbing)\n\n\n# Inner join keeps only counties that appear in both years\ncomp &lt;- inner_join(wi23, wi22, by = c(\"geoid\", \"name\"))\n\n\nThis a quick exploratory summary of each year.\n\n\nCode\n# This gives you a sense of the distribution in each year.\ncomp |&gt;\n  select(p22, p23) |&gt;\n  summary()\n\n\n      p22               p23         \n Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.02091   1st Qu.:0.02228  \n Median :0.05467   Median :0.06044  \n Mean   :0.09782   Mean   :0.10166  \n 3rd Qu.:0.10753   3rd Qu.:0.11712  \n Max.   :3.72906   Max.   :3.91322  \n NA's   :2         NA's   :1        \n\n\n\n\nCode\nggplot(comp, aes(x = p22, y = p23)) +\n  geom_point(alpha = 0.6, size = 1.2) +              # scatter points\n  geom_smooth(method = \"lm\", se = FALSE) +           # simple trend line\n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.01)) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 0.01)) +\n  labs(\n    title = \"U.S. Counties: Percent Lacking Complete Indoor Plumbing\",\n    subtitle = \"Comparing 2023 vs. 2022 ACS 1-year Estimates\",\n    x = \"2022 % lacking plumbing\",\n    y = \"2023 % lacking plumbing\",\n    caption = \"Source: U.S. Census Bureau, ACS 1-year Estimates (tidycensus)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nIn this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nReferences:\nTidyTuesday Source: R4DS Online Learning Community. (2025). TidyTuesday: ACS Plumbing and Population Data (2025-01-28). TidyTuesday project. Retrieved from https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-01-28/readme.md\nOriginal Data Source: U.S. Census Bureau. (2022–2023). American Community Survey (ACS) 1-year Estimates, Tables B01003 (Total Population) and B25049 (Plumbing Facilities). Washington, DC: U.S. Department of Commerce. Retrieved from https://www.census.gov/programs-surveys/acs/data.html"
  },
  {
    "objectID": "tidydata2.html",
    "href": "tidydata2.html",
    "title": "Federations With the Highest Population of 2000+ rated players in September 2025",
    "section": "",
    "text": "The September 2025 TidyTuesday dataset contains FIDE (International Chess Federation) rating lists for registered competitive chess players worldwide. Each entry includes a player’s country federation, FIDE ID, standard rating, titles, and demographic information. In this analysis, I focus on identifying which national federations have the largest number of strong tournament players, defined here as players rated 2000 or higher in standard/classical chess.\n\n\nCode\nlibrary(tidytuesdayR)\ntuesdata &lt;- tt_load('2025-09-23')\n\nfide_ratings_september &lt;- tuesdata$fide_ratings_september #Extracts the data for September\n\n\nHere’s a glimpse at what this data set is working with, contextualizing the explanation above.\n\n\nCode\nhead(fide_ratings_september) #Gives us a peak into the variables of the data set\n\n\n# A tibble: 6 × 12\n        id name   fed   sex   title wtitle otitle foa   rating games     k  bday\n     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 53707043 A Dar… IND   M     &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;    1412     0    40  2013\n2 53200465 A F M… BAN   M     &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;    1797     0    40  1977\n3  5716365 A Ham… MAS   M     &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;    1552     0    20  1970\n4 53200553 A I S… BAN   M     &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;    1607     0    40  1995\n5  5045886 A K, … IND   M     &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;    1747     0    20  1964\n6 10291695 A S M… BAN   M     &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;    1614     0    40  2008\n\n\nThis is the list of Federations with the highest number of 2000+ players in descending order\n\n\nCode\nlibrary(tidyverse)\n\nplayers_2000 &lt;- fide_ratings_september |&gt;\n  filter(!is.na(rating)) |&gt;  # keep only players with a real rating\n  filter(rating &gt;= 2000)     # keep only players with a rating of 2000+\n\n#This counts players per federation and extracts the top 5\ntop_5_feds &lt;- players_2000 |&gt; \n  count(fed, name = \"n_players_2000plus\", sort = TRUE) |&gt;\n  slice_max(n_players_2000plus, n = 5)\n\ntop_5_feds\n\n\n# A tibble: 5 × 2\n  fed   n_players_2000plus\n  &lt;chr&gt;              &lt;int&gt;\n1 GER                 3235\n2 ESP                 2215\n3 FRA                 1409\n4 NED                 1064\n5 CZE                 1051\n\n\n\n\nCode\n#|fig-alt: \"Horizontal bar chart showing the number of FIDE chess players with ratings of 2000 or higher in the five federations with the largest such populations.\"\n\ntop_5_feds |&gt;\n  mutate(fed = fct_reorder(fed, n_players_2000plus)) |&gt;\n  ggplot(aes(x = fed, y = n_players_2000plus)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Top 5 Federations by Number of 2000+ Rated FIDE Players\",\n    x     = \"Federation\",\n    y     = \"Number of 2000+ rated players\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nThis plot shows, for the September 2025 FIDE rating list, the five federations with the largest number of players rated 2000 or higher. Germany (GER) has the highest count of strong players, followed by Spain (ESP) and France (FRA), with the Netherlands (NED) and the Czech Republic (CZE) rounding out the top five. The differences in bar length indicate that Germany and Spain have substantially more 2000+ rated players than the other federations in this group, reflecting deeper pools of competitive players in those national systems. It’s important to remember that these counts come from the official FIDE ratings list at a single point in time, not from all casual chess players in each country.\nReferences: Original Data: International Chess Federation (FIDE). FIDE Rating Lists – September 2025. FIDE Ratings. Retrieved from https://ratings.fide.com/download_lists.phtml.\nTidy Tuesday Source: R4DS Online Learning Community. (2025). TidyTuesday: FIDE Ratings (2025-09-23). TidyTuesday project. Retrieved from https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-09-23/readme.md."
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Patterns in Netflix Shows",
    "section": "",
    "text": "This project looks at the TidyTuesday Netflix Titles dataset from 2021-04-20, which includes metadata and short descriptions for movies and TV shows. In my plots, I compare movie runtimes by decade and title lengths across shows in the data set.\nMy analysis focuses on three small exploratory tasks:\n\nComparing movie runtimes across decades\nExamining the distribution of title lengths\nExploring the number of seasons in Netflix TV shows\n\n\n\nThe cleaned data set contains the type (movie or tv show), it’s title, the date it was added to the platform, when it originally released, its duration in seasons or minutes, and a description of the plot.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\n\n\nnetflix &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv\",\n)\n\n# Removing irrelevant columns & creating a decade variable\nclean_data &lt;- netflix |&gt;\n  select(type, title, date_added, release_year, duration, description) |&gt;\n  mutate(\n    title       = str_squish(title),\n    description = str_squish(description),\n    decade      = (release_year %/% 10) * 10\n  )\nhead(clean_data)\n\n\n# A tibble: 6 × 7\n  type    title date_added        release_year duration  description      decade\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;\n1 TV Show 3%    August 14, 2020           2020 4 Seasons In a future whe…   2020\n2 Movie   7:19  December 23, 2016         2016 93 min    After a devasta…   2010\n3 Movie   23:59 December 20, 2018         2011 78 min    When an army re…   2010\n4 Movie   9     November 16, 2017         2009 80 min    In a postapocal…   2000\n5 Movie   21    January 1, 2020           2008 123 min   A brilliant gro…   2000\n6 TV Show 46    July 1, 2017              2016 1 Season  A genetics prof…   2010\n\n\nThe duration column in the original dataset is stored as text, with movies coded like “95 min” and TV shows coded like “3 seasons” . To analyze runtimes and season counts numerically, I use regular expressions to pull out just the digits at the beginning of these strings. For movies, I look for a number followed by “min” and convert it to a minutes variable. For TV shows, I look for a number followed by “Season” or “Seasons” or and convert it to a seasons variable. This lets me treat runtimes and season counts as numbers.\n\n\nShow code\n# Regex parses runtimes\nparsed_data &lt;- clean_data |&gt;\n  mutate(\n     # Movies store duration in minutes\n    minutes = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*min$)\")),\n\n    # TV Shows store seasons\n    seasons = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*Season(?:s)?$)\")) \n  )\n\n\n\n\nShow code\n#| fig-alt: \"Density curves comparing the distribution of movie runtimes across release decades, showing differences in typical runtime length over time.\"\n\n\nparsed_data |&gt;\n  filter(type == \"Movie\", !is.na(minutes), !is.na(decade)) |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  ggplot(aes(x = minutes, fill = decade)) +\n  geom_density(alpha = 0.25) +\n  labs(\n    title = \"Movie runtime distributions by decade\",\n    x = \"Runtime (minutes)\",\n    y = \"Density\",\n    fill = \"Decade\"\n  )\n\n\n\n\n\n\n\n\n\nEach curve on this plot shows the distribution of Netflix movie runtimes for each decade represented since the 40s. The higher peaks show the most common lengths for that decade. What we see in this plot are the changes across time for what a typical runtime looks like in each era, with the peaks showing what the most frequent lengths were for a given decade.\n\n\nShow code\nparsed_data |&gt;\n  transmute(\n    title_words = title |&gt;\n      str_squish() |&gt;\n      str_count(\"\\\\S+\")  # count non-space sequences         \n  ) |&gt;\n  filter(!is.na(title_words)) |&gt;\n  ggplot(aes(x = title_words)) +\n  geom_histogram(binwidth = 1, center = 0.5) +\n  labs(\n    title = \"How long are Netflix titles?\",\n    subtitle = \"Histogram of the number of words per title\",\n    x = \"Words in title\",\n    y = \"Number of titles\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar on this plot shows how many Netflix titles have a given number of words in their names. Taller bars mark the most common title lengths. What we see is that short, punchy names (around 1–3 words) are most frequent, with a tapering tail of longer titles that tend to appear less often.\n\n\nShow code\nparsed_data |&gt;\n  filter(type == \"TV Show\", !is.na(seasons), seasons &lt;= 11) |&gt; # limit outliers\n  count(seasons, sort = FALSE) |&gt;\n  mutate(seasons = factor(seasons, levels = sort(unique(seasons)))) |&gt;\n  ggplot(aes(x = seasons, y = n)) +\n  geom_col() +\n  labs(\n    title = \"How many seasons do Netflix TV shows have?\",\n    subtitle = \"Counts of TV shows by stated season count (shows with &gt;11 seasons excluded)\",\n    x = \"Seasons\",\n    y = \"Number of TV shows\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar shows how many Netflix TV shows report a given number of seasons, 1 - 11. The tallest bars at the low season counts show that most Netflix series in the catalog are shorter/only have 1 season. This plot excluded series with more than 11 seasons for readability.\nReferences:\n\nTidyTuesday – Netflix Titles (2021-04-20)\nrfordatascience/tidytuesday (CSV + README).\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-04-20\nOriginal Dataset – Netflix Movies and TV Shows (Kaggle)\nCommunity-compiled catalog commonly cited as the underlying source.\nhttps://www.kaggle.com/datasets/shivamb/netflix-shows"
  },
  {
    "objectID": "Project_4.html",
    "href": "Project_4.html",
    "title": "Algorithmic bias from AFST to Predictive Policing",
    "section": "",
    "text": "The Allegheny Family Screening Tool (AFST) is an algorithm used by Allegheny County, Pennsylvania to help child welfare workers decide whether to investigate a hotline call about child neglect. When a report is filed, the system automatically pulls information from multiple public databases—welfare benefits, criminal justice involvement, juvenile probation, mental health records, and public school data—and uses that information to generate a risk score from 1 to 20, meant to predict the likelihood that a child will be removed from the home within two years.\nAccording to the Pulitzer Center (Ho & Burke, 2022), the AFST was developed to help caseworkers manage high call volumes, reduce human bias, and allocate agency resources efficiently. The system is often described as a neutral, objective supplemental tool designed to assist human decision-making. However, as Ballantyne (2023) argues, the algorithm operates in a high-stakes context—its predictions can shape whether families experience intrusive investigations, heightened state surveillance, or even the possibility of temporary or permanent child removal. This combination of administrative data, predictive modeling, and discretionary state power makes the AFST a powerful example of the data science ethical dilemmas highlighted in Data Feminism, including the central questions: Who benefits? Who is harmed? and Whose priorities are embedded in the system?\nIn what follows, I examine four core ethical issues—consent, representativeness, unintended data use, and racial proxies—and analyze how each plays out within this human-algorithm collaboration between social workers and algorithmic predictions.\n\n\n\nThe AFST raises significant concerns about whether meaningful consent is possible. Families whose information populates the system never knowingly agreed to have their welfare, education, or criminal justice records combined into an algorithmic decision-making tool. The Pulitzer Center article emphasizes that most families do not even know the algorithm exists, much less that it influences whether a caseworker opens an investigation into their home (Ho & Burke, 2022). This means that, despite the deeply personal nature of the data involved, individuals cannot control how their information is used, cannot opt out, and cannot reasonably understand how ordinary interactions with public systems—such as applying for food stamps or receiving school attendance letters—may later be treated as risk indicators.\nBallantyne (2023) further notes that consent is complicated by the inherent power imbalance between families and the state agencies that collect their data. Families interacting with welfare, education, or criminal justice systems typically have no choice but to provide information as a condition of receiving services or complying with legal requirements. This is not voluntary participation in research—it is coerced disclosure embedded in structural inequality. Therefore, informed consent is not only absent; the very possibility of informed consent is undermined by the conditions under which the data are collected.\n\n\n\nA core issue is who appears in the dataset. The AFST trains exclusively on data from families who interact with public systems, meaning poor and disproportionately Black families are the ones most heavily monitored. The Pulitzer Center makes clear that the algorithm draws from welfare records, school data, public mental health services, and criminal justice involvement—systems that are not evenly distributed across socioeconomic lines (Ho & Burke, 2022). Families with higher income who use private healthcare, private schools, or do not rely on public benefits leave far fewer digital traces and therefore remain largely invisible to the model.\nThis creates a severe representativeness problem. The algorithm purports to assess risk for children across the county, but in practice, it can only “see” those who are already hyper-surveilled through state systems. As Ballantyne (2023) points out, the model does not measure underlying risk; it measures exposure to public institutions. This means the AFST may conflate poverty with danger, treating structural disadvantage as evidence of impending neglect. From the perspective of Data Feminism, this demonstrates how power operates—the groups already monitored by the state become the objects of even greater scrutiny.\n\n\n\nThe AFST’s use of administrative data illustrates a classic case of function creep, where information collected for one purpose is repurposed for a different, often more punitive, goal. Welfare records were originally gathered to distribute benefits, school records to track educational progress, and criminal justice data to document legal processes. But according to the Pulitzer Center, this disparate information is now merged to generate family risk scores (Ho & Burke, 2022). The algorithm reframes routine or structural conditions—such as unemployment, food insecurity, or living in a heavily policed neighborhood—as potential indicators of future child maltreatment.\nBallantyne (2023) argues that this repurposing fundamentally changes the meaning of the data: information that once signaled a need for support now becomes evidence the state could act upon. This raises ethical concerns about whether individuals should be subject to predictive analysis based on data they provided for unrelated administrative reasons. The recontextualization of data, without public awareness or consent, expands the reach of state surveillance under the guise of efficiency.\n\n\n\nEven though race is not explicitly included as a variable, the AFST incorporates numerous proxy variables that reflect racial disparities. Variables such as zip code, welfare usage, school suspensions, juvenile probation, and arrest records are deeply correlated with race due to long-standing structural inequities. Ho & Burke note that these patterns cause families of color—especially Black families—to receive disproportionately higher risk scores (Ho & Burke, 2022). Removing the race variable does not remove racial influence; it only hides it behind factors shaped by residential segregation, policing practices, and unequal access to resources.\nBallantyne (2023) points out that the algorithm’s design reinforces existing inequities by reproducing the racial patterns embedded in the data. Instead of addressing the conditions that shape the data (poverty, discrimination, over-policing), the tool treats these structural forces as individual-level risk factors. This mirrors a key question from the Data Feminism framework: Is the issue really “bias in the algorithm,” or is the algorithm simply reflecting unjust systems that already exist?\n\n\n\nThe AFST illustrates how data science systems can shift power toward institutions and away from the people they govern. Who benefits? The county and its caseworkers gain efficiency and predictive tools that help allocate scarce resources. Who is harmed? Families who are already marginalized face heightened surveillance, invasive investigations, and greater risk of family separation. As Ballantyne (2023) emphasizes, even the best-intentioned human–AI collaborations can cause harm when they reinforce unequal power structures. And as the Pulitzer Center shows, the ethical dilemmas here are not hypothetical—the algorithm’s risk scores shape real decisions with life-altering consequences.\nUltimately, this case reflects the broader pattern described in Data Feminism: data science is often used “in the service of surveillance (of the minoritized) and efficiency (amidst scarcity).” The AFST is not just a technical tool; it is a form of governance that makes certain families more visible to the state while others remain unseen. It embeds social inequalities into mathematical form, presenting them as objective truth. This matters because data-driven decisions often carry an aura of neutrality that masks the power relations beneath. Understanding these dynamics is crucial if data science is to be used ethically and responsibly in social systems.\nReferences:\nCase 1 References\nBallantyne, N. (2023). The harm that data do: The case of the Allegheny Family Screening Tool. Medium. https://medium.com/@neilballantyne/the-harm-that-data-do-the-case-of-the-allegheny-family-screening-tool-5f9fca22e0b2\nHo, S., & Burke, G. (2022). An algorithm that screens for child neglect raises concerns. Pulitzer Center. https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns\n\nD’Ignazio, C., & Klein, L. F. (2020). Data Feminism. MIT Press."
  },
  {
    "objectID": "Project_4.html#case-1-the-allegheny-family-screening-tool-and-the-ethics-of-predictive-welfare-surveillance",
    "href": "Project_4.html#case-1-the-allegheny-family-screening-tool-and-the-ethics-of-predictive-welfare-surveillance",
    "title": "Algorithmic bias from AFST to Predictive Policing",
    "section": "",
    "text": "The Allegheny Family Screening Tool (AFST) is an algorithm used by Allegheny County, Pennsylvania to help child welfare workers decide whether to investigate a hotline call about child neglect. When a report is filed, the system automatically pulls information from multiple public databases—welfare benefits, criminal justice involvement, juvenile probation, mental health records, and public school data—and uses that information to generate a risk score from 1 to 20, meant to predict the likelihood that a child will be removed from the home within two years.\nAccording to the Pulitzer Center (Ho & Burke, 2022), the AFST was developed to help caseworkers manage high call volumes, reduce human bias, and allocate agency resources efficiently. The system is often described as a neutral, objective supplemental tool designed to assist human decision-making. However, as Ballantyne (2023) argues, the algorithm operates in a high-stakes context—its predictions can shape whether families experience intrusive investigations, heightened state surveillance, or even the possibility of temporary or permanent child removal. This combination of administrative data, predictive modeling, and discretionary state power makes the AFST a powerful example of the data science ethical dilemmas highlighted in Data Feminism, including the central questions: Who benefits? Who is harmed? and Whose priorities are embedded in the system?\nIn what follows, I examine four core ethical issues—consent, representativeness, unintended data use, and racial proxies—and analyze how each plays out within this human-algorithm collaboration between social workers and algorithmic predictions.\n\n\n\nThe AFST raises significant concerns about whether meaningful consent is possible. Families whose information populates the system never knowingly agreed to have their welfare, education, or criminal justice records combined into an algorithmic decision-making tool. The Pulitzer Center article emphasizes that most families do not even know the algorithm exists, much less that it influences whether a caseworker opens an investigation into their home (Ho & Burke, 2022). This means that, despite the deeply personal nature of the data involved, individuals cannot control how their information is used, cannot opt out, and cannot reasonably understand how ordinary interactions with public systems—such as applying for food stamps or receiving school attendance letters—may later be treated as risk indicators.\nBallantyne (2023) further notes that consent is complicated by the inherent power imbalance between families and the state agencies that collect their data. Families interacting with welfare, education, or criminal justice systems typically have no choice but to provide information as a condition of receiving services or complying with legal requirements. This is not voluntary participation in research—it is coerced disclosure embedded in structural inequality. Therefore, informed consent is not only absent; the very possibility of informed consent is undermined by the conditions under which the data are collected.\n\n\n\nA core issue is who appears in the dataset. The AFST trains exclusively on data from families who interact with public systems, meaning poor and disproportionately Black families are the ones most heavily monitored. The Pulitzer Center makes clear that the algorithm draws from welfare records, school data, public mental health services, and criminal justice involvement—systems that are not evenly distributed across socioeconomic lines (Ho & Burke, 2022). Families with higher income who use private healthcare, private schools, or do not rely on public benefits leave far fewer digital traces and therefore remain largely invisible to the model.\nThis creates a severe representativeness problem. The algorithm purports to assess risk for children across the county, but in practice, it can only “see” those who are already hyper-surveilled through state systems. As Ballantyne (2023) points out, the model does not measure underlying risk; it measures exposure to public institutions. This means the AFST may conflate poverty with danger, treating structural disadvantage as evidence of impending neglect. From the perspective of Data Feminism, this demonstrates how power operates—the groups already monitored by the state become the objects of even greater scrutiny.\n\n\n\nThe AFST’s use of administrative data illustrates a classic case of function creep, where information collected for one purpose is repurposed for a different, often more punitive, goal. Welfare records were originally gathered to distribute benefits, school records to track educational progress, and criminal justice data to document legal processes. But according to the Pulitzer Center, this disparate information is now merged to generate family risk scores (Ho & Burke, 2022). The algorithm reframes routine or structural conditions—such as unemployment, food insecurity, or living in a heavily policed neighborhood—as potential indicators of future child maltreatment.\nBallantyne (2023) argues that this repurposing fundamentally changes the meaning of the data: information that once signaled a need for support now becomes evidence the state could act upon. This raises ethical concerns about whether individuals should be subject to predictive analysis based on data they provided for unrelated administrative reasons. The recontextualization of data, without public awareness or consent, expands the reach of state surveillance under the guise of efficiency.\n\n\n\nEven though race is not explicitly included as a variable, the AFST incorporates numerous proxy variables that reflect racial disparities. Variables such as zip code, welfare usage, school suspensions, juvenile probation, and arrest records are deeply correlated with race due to long-standing structural inequities. Ho & Burke note that these patterns cause families of color—especially Black families—to receive disproportionately higher risk scores (Ho & Burke, 2022). Removing the race variable does not remove racial influence; it only hides it behind factors shaped by residential segregation, policing practices, and unequal access to resources.\nBallantyne (2023) points out that the algorithm’s design reinforces existing inequities by reproducing the racial patterns embedded in the data. Instead of addressing the conditions that shape the data (poverty, discrimination, over-policing), the tool treats these structural forces as individual-level risk factors. This mirrors a key question from the Data Feminism framework: Is the issue really “bias in the algorithm,” or is the algorithm simply reflecting unjust systems that already exist?\n\n\n\nThe AFST illustrates how data science systems can shift power toward institutions and away from the people they govern. Who benefits? The county and its caseworkers gain efficiency and predictive tools that help allocate scarce resources. Who is harmed? Families who are already marginalized face heightened surveillance, invasive investigations, and greater risk of family separation. As Ballantyne (2023) emphasizes, even the best-intentioned human–AI collaborations can cause harm when they reinforce unequal power structures. And as the Pulitzer Center shows, the ethical dilemmas here are not hypothetical—the algorithm’s risk scores shape real decisions with life-altering consequences.\nUltimately, this case reflects the broader pattern described in Data Feminism: data science is often used “in the service of surveillance (of the minoritized) and efficiency (amidst scarcity).” The AFST is not just a technical tool; it is a form of governance that makes certain families more visible to the state while others remain unseen. It embeds social inequalities into mathematical form, presenting them as objective truth. This matters because data-driven decisions often carry an aura of neutrality that masks the power relations beneath. Understanding these dynamics is crucial if data science is to be used ethically and responsibly in social systems.\nReferences:\nCase 1 References\nBallantyne, N. (2023). The harm that data do: The case of the Allegheny Family Screening Tool. Medium. https://medium.com/@neilballantyne/the-harm-that-data-do-the-case-of-the-allegheny-family-screening-tool-5f9fca22e0b2\nHo, S., & Burke, G. (2022). An algorithm that screens for child neglect raises concerns. Pulitzer Center. https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns\n\nD’Ignazio, C., & Klein, L. F. (2020). Data Feminism. MIT Press."
  },
  {
    "objectID": "Project_4.html#case-2-nyc-stop-and-frisk-and-predictive-policing.",
    "href": "Project_4.html#case-2-nyc-stop-and-frisk-and-predictive-policing.",
    "title": "Algorithmic bias from AFST to Predictive Policing",
    "section": "CASE 2: NYC, Stop and Frisk, and Predictive Policing.",
    "text": "CASE 2: NYC, Stop and Frisk, and Predictive Policing.\nThis analysis explores the distribution of NYPD stop-and-frisk encounters across racial groups using publicly reported 2024 data. This approach highlights who appears in the administrative record, which is essential for understanding how predictive systems—whether in policing or child welfare—learn patterns of state surveillance rather than neutral patterns of risk.For this case, I use the 2024 NYPD Stop-and-Frisk (SQF) dataset, which contains one row for each recorded police stop. For our purposes, the most relevant information from the data is:\n\nThe suspect’s race (as recorded by the officer)\nWhether the person was frisked\nWhether they were searched\nWhether they were arrested\nWhether a weapon was found\n\nHowever, the data also includes information about the officers, the locations of stops, probable cause, source of officer suspicion, consent, outcomes, whether or not the stop was self-initiated by the officer, and more.\nThis type of administrative policing data is the same kind of record that often feeds predictive policing systems. By examining who appears in the data and how outcomes are distributed across race, we can see how predictive models learn patterns of surveillance, not neutral patterns of “crime risk.”\n\nHere, I create a cleaner version of the dataset for my purposes. In doing so, I pull the race description of the suspects, Y/N stop outcomes, and drop N/A values\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\n\nsf &lt;- read_excel(\"sqf-2024.xlsx\")\n\n#creates new columns with standardized variable names \nsf_clean &lt;- sf |&gt;\n  mutate(\n    race   = SUSPECT_RACE_DESCRIPTION,\n    frisk  = FRISKED_FLAG == \"Y\",\n    search = SEARCHED_FLAG == \"Y\",\n    arrest = SUSPECT_ARRESTED_FLAG == \"Y\",\n    weapon = WEAPON_FOUND_FLAG == \"Y\"\n  ) |&gt;\n\n  #drop missing race labels so categories are interpretable\n  filter(!is.na(race), race != \"(null)\")\n\n\n\nNext, I summarized the data by race. For each group, I compute the total number of stops, the number of frisks, searches, and arrests conducted by officers, and the proportion of city wide occurrences within each category for a given race.\n\n\nCode\nsummary_race &lt;- sf_clean |&gt;\n  group_by(race) |&gt;\n  summarize(\n    \n    #Provides the total number of stops, frisks, searches, arrests, and weapons found by race\n    stops = n(),\n    frisks = sum(frisk,   na.rm = TRUE),\n    searches = sum(search,  na.rm = TRUE),\n    arrests = sum(arrest,  na.rm = TRUE),\n    weapons = sum(weapon,  na.rm = TRUE)\n  ) |&gt;\n  mutate(\n    \n    # Provides the proportion of each occurence by race\n    prop_stops = stops    / sum(stops),\n    prop_frisks = frisks   / sum(frisks),\n    prop_searches = searches / sum(searches),\n    prop_arrests = arrests  / sum(arrests),\n    prop_weapons = weapons  / sum(weapons)\n  ) |&gt;\n  arrange(desc(prop_stops))\n\n\n\n\nStop rate by race:\nThis plot shows what share of all 2024 NYPD stops involve suspects of each recorded race.\n\n\nCode\nsummary_race |&gt;\n  ggplot(aes(x = reorder(race, prop_stops), y = prop_stops)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Share of All NYPD Stops by Race — 2024\",\n    x = \"Race\",\n    y = \"Proportion of All Stops\",\n    caption = \"Data: NYPD Stop-and-Frisk 2024\"\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows the distribution of NYPD stop-and-frisk encounters across racial groups in 2024. Black and Hispanic people together make up the largest share of recorded stops in the dataset. This does not tell us why these patterns occur, but it does illustrate an important point: administrative policing data reflect where and with whom enforcement activity is documented, not necessarily where underlying risk or harm exists.\nFor predictive policing systems trained on data like these (of which the majority are), the model would primarily learn from these recorded patterns. This makes the dataset itself a useful example of how certain groups or neighborhoods can become more visible to algorithmic tools simply because they appear more often in historical records.\n\n\nSearch Rate by Race\nThis plot uses the same summary table but focuses on searches, not just stops. It shows what share of all recorded searches involve each racial group\n\n\nCode\nsummary_race |&gt;\n  ggplot(aes(x = reorder(race, prop_searches), y = prop_searches)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Share of All NYPD Searches by Race — 2024\",\n    x = \"Race\",\n    y = \"Proportion of All Searches\",\n    caption = \"Data: NYPD Stop-and-Frisk 2024\"\n  )\n\n\n\n\n\n\n\n\n\nThe distribution of searches looks similar to the distribution of stops: Black and Hispanic people make up a substantial proportion of the recorded search activity. Again, these data do not reveal the reasons behind these outcomes.\nHowever, they do show that searches — which represent a more intrusive form of police contact — occur most frequently among groups that also appear most often in stop records. For predictive policing models, this matters because such systems learn from patterns of observed enforcement, not necessarily from objective measures of risk. The result is that historical enforcement patterns become part of the model’s training environment.\n\n\nArrest rate by race\nThis plot looks at arrests among all stop-and-frisk encounters in 2024. It shows what proportion of all recorded arrests involve each race.\n\n\nCode\nsummary_race |&gt;\n  ggplot(aes(x = reorder(race, prop_arrests), y = prop_arrests)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Share of All NYPD Arrests by Race — 2024\",\n    x = \"Race\",\n    y = \"Proportion of All Arrests\",\n    caption = \"Data: NYPD Stop-and-Frisk 2024\"\n  )\n\n\n\n\n\n\n\n\n\nThis plot looks at arrests following stop-and-frisk encounters. The racial distribution of arrests largely mirrors the distribution of stops and searches. As before, these data cannot tell us why arrests occur at these rates or what factors drive police decision-making.\nWhat the plot does illustrate is that recorded enforcement outcomes — the events that would form the training signal in a predictive system — are not evenly distributed across racial groups. This makes the dataset a clear example of how predictive policing models can inherit patterns present in administrative records, even when the underlying causes of those patterns are complex.\n\n\n6. Zooming In: Arrest Rate for Black Stops\nTo dig deeper, I subset to Black individuals only and compute the total number of stops of Black people, gow many of those stops result in arrest, and how many do not result in arrest. From there I reshape this summary to make a simple two-category plot: arrested vs not arrested\n\n\nCode\nblack_df &lt;- sf_clean |&gt;\n  filter(race == \"BLACK\") |&gt;\n  summarize(\n    black_stops = n(),\n    black_arrests = sum(arrest, na.rm = TRUE)\n  ) |&gt;\n  mutate(\n    black_non_arrests = black_stops - black_arrests\n  )\n\nblack_plot &lt;- black_df |&gt;\n  pivot_longer(\n    cols = c(black_arrests, black_non_arrests),\n    names_to = \"outcome\",\n    values_to = \"count\"\n  ) |&gt;\n  mutate(\n    outcome = if_else(outcome == \"black_arrests\", \"Arrests\", \"Not Arrested\")\n  )\n\n\n\n\nCode\n#| fig-alt: \"Pie chart showing the proportion of Black NYPD stops that resulted in arrest. \nblack_plot |&gt;\n  mutate(\n    percent = count / sum(count)\n  ) |&gt;\n  ggplot(aes(x = \"\", y = percent, fill = outcome)) +\n  geom_col(width = 1) +\n  geom_text(aes(label = scales::percent(percent, accuracy = 0.1)),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(theta = \"y\") +\n  labs(\n    title = \"What Share of Black Stops Resulted in Arrests? — NYPD 2024\",\n    fill = \"\",\n    caption = \"Data: NYPD Stop-and-Frisk 2024\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nFocusing on Black New Yorkers only, this plot shows what share of stops resulted in arrest compared to stops that did not. Most stops of Black people in 2024 did not lead to an arrest.\nThis does not explain why officers make stops or arrests, but it highlights a feature of the underlying data: many enforcement encounters do not result in a criminal charge. For predictive systems, this matters because such models treat all recorded stops as data points, regardless of outcome.\nThe dataset therefore serves as a microcosm of how administrative policing data function: they document where enforcement occurs, not necessarily where risk exists. Because predictive models are trained on these records, the patterns present in them — including disparities — can be learned and reproduced by algorithmic tools.\n\n\nWhy this matters\n\nPredictive Policing: Why Biased Administrative Data Leads to Biased Algorithmic Outputs\nPredictive policing systems are often described as “data-driven tools” that forecast where crime is likely to occur or which people might be at elevated “risk.” In practice, these systems learn from exactly the kinds of administrative records I visualized above: police stops, searches, arrests, and incident reports. The underlying idea is simple, historical enforcement patterns will help forecast future ones, but the consequences are more complicated.\nTools like PredPol (now Geolitica) take police-generated data (locations of arrests, reported incidents, and sometimes field stops) and feed them into statistical forecasting models designed to identify spatiotemporal clusters (NIJ Crime Solutions). The model returns small hotspot boxes on a map where officers are sent to patrol. However, as Lum & Isaac argue, the model does not discover where crime is happening, but learns patterns in the data it is given, and those data primarily reflect policing behavior rather than underlying crime patterns (Lum & Isaac, 2016).\nThis connects directly to what the NYPD data show. Black New Yorkers constitute a disproportionately high share of 2024 stops, searches, and arrests. That doesn’t, by itself, explain why these disparities exist, but it does tell us that police databases are not neutral snapshots of crime. These administrative records are simply reflections of state behavior. If these same administrative traces form the training data for a predictive system, the model will inevitably learn that “risk” is concentrated where enforcement has historically been concentrated.\nResearchers have shown that this dynamic creates a feedback loop:\n\nPolice concentrate enforcement in particular neighborhoods or among specific groups.\nThose encounters become the data used to train the predictive model.\nThe model “discovers” elevated risk in those same places or among those same groups.\nPolice are sent back to patrol those areas.\nNew stops and arrests reinforce the pattern, regardless of changes in underlying crime (Lum & Isaac, 2016).\n\n\nA 2023 evaluation of Geolitica’s deployment in Plainfield, New Jersey found that fewer than 0.5% of its predictions matched any reported crime—yet officers still patrolled according to the algorithm’s guidance, shaping future data regardless of predictive success (Kofman & Scheiber, 2023). Even when accuracy is low, the tool’s outputs influence institutional behavior.\n\n\nPerson-Based Prediction: When Administrative Traces Follow People\nSome predictive-policing systems generate individual “risk scores” rather than geographic hotspots. Chicago’s Strategic Subject List (SSL) ranked residents based on prior arrests, prior victimization, alleged gang affiliation, and similar variables. Later assessments found that many people the SSL flagged were more likely to be arrested without being convicted, suggesting that the list increased enforcement without improving safety (Joh, 2017; Koepke, 2016).\nLos Angeles’s Operation LASER and its PredPol contract were ultimately ended after audits raised concerns about inconsistent data, limited transparency, and discriminatory impacts (Brennan Center for Justice, 2020).\nThe crucial point is that these systems rely on the same kinds of administrative traces as the NYPD dataset. If roughly a quarter of stops of Black individuals result in arrests (as the pie chart above shows), those arrests become future inputs. They accumulate into risk profiles used for later prediction.\n\n\nEthical Stakes: Why Algorithms Amplify, Rather Than Correct, Institutional Patterns\nTaken together, these findings reflect a broader ethical concern:\n\nThe datasets used to train predictive systems already encode racialized patterns of policing.\nThe model treats those historical patterns as evidence of future risk.\nThe outputs reinforce the same institutional behaviors.\n\nFrom a Data Feminism perspective, predictive policing centers institutional goals—efficiency, coverage, the appearance of precision—while minimizing attention to community harms (D’Ignazio & Klein, 2020). When a model learns from stop-and-frisk data, it is not discovering objective truths about crime. It is learning the history of surveillance, and then operationalizing that history as if it were neutral.\nThis dynamic mirrors the AFST example in Case 1. In both contexts:\n\nThe algorithm learns from patterns of state intervention—not underlying social needs or behaviors.\nPeople and communities already made visible to the state become even more visible in the algorithmic risk space.\nThe output reinforces the very inequalities embedded in the training data.\n\n\nIn that sense, predictive policing and the AFST share a common structure: neither reveals hidden insight. Instead, both systems weaponize institutional history. The NYPD data above reveal exactly how that history is recorded, and they show why any predictive tool that trains on these records will inevitably reproduce the same patterns\nCase 2 References:\nBrennan Center for Justice. (2020). Predictive policing today: A shared statement of concern. https://www.brennancenter.org/our-work/research-reports/predictive-policing-today\nD’Ignazio, C., & Klein, L. F. (2020). Data Feminism. MIT Press.\nJoh, E. E. (2017). Feeding the machine: Policing, crime data, & algorithms. William & Mary Bill of Rights Journal, 26, 287–330.\nKofman, A., & Scheiber, N. (2023). Predictive policing software terrible at predicting crimes. The Markup. https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes\nKoepke, L. (2016). Stuck in a pattern: Early evidence on “predictive policing” and civil rights. Upturn. https://www.teamupturn.com/reports/2016/stuck-in-a-pattern\nLum, K., & Isaac, W. (2016). To predict and serve? Significance, 13(5), 14–19. https://doi.org/10.1111/j.1740-9713.2016.00960.x\nNational Institute of Justice. Predictive policing evaluation—CrimeSolutions.gov. https://crimesolutions.ojp.gov\nNew York City Police Department. (2024). Stop, Question, and Frisk Data, 2024. NYC Open Data / NYPD Reports. https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page"
  },
  {
    "objectID": "Project_5.html",
    "href": "Project_5.html",
    "title": "Variations in Stop Trends in Oakland, San Francisco, and San Diego Along Racial Lines",
    "section": "",
    "text": "Show code\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\n# Connect to the shared traffic database using credentials\ncon_traffic &lt;- dbConnect(\nRMariaDB::MariaDB(),\ndbname = \"traffic\",\nhost = Sys.getenv(\"TRAFFIC_HOST\"),\nuser = Sys.getenv(\"TRAFFIC_USER\"),\npassword = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nThis project seeks to interrogate 2 questions: how do traffic stop patterns differ across California cities, and how do these patterns change when we examine different types of stops within a single city? To answer these questions, I compare the racial distribution of police stops in San Diego and San Francisco, and then investigate how Oakland’s vehicular and pedestrian stops vary across racial groups.\n\nStops by Race: San Diego vs San Francisco\nThe goal of this first analysis is to compare how police stops are distributed across racial groups in two major California jurisdictions, San Diego and San Francisco. Both cities report similar variables, which makes them suitable for direct comparison. By focusing on stop counts by race, this section establishes a baseline understanding of how each city’s overall traffic-stop population is composed before exploring more detailed stop characteristics in the second visualization.\nHere, the SQL query below pulls stop counts by recorded subject race for each city.\n\n\nShow code\n\n# Count stops by race in San Diego\nSELECT\n  'San Diego' AS city, -- label for this table\n  subject_race, -- race recorded in the stop\n  COUNT(*) AS n_stops -- number of stops per race\nFROM ca_san_diego_2020_04_01\nWHERE subject_race IN ('asian/pacific islander','black','hispanic','other','white')\nGROUP BY subject_race\n\nUNION ALL\n\n# Count stops by race in San Francisto \nSELECT\n  'San Francisco' AS city,\n  subject_race,\n  COUNT(*) AS n_stops\nFROM ca_san_francisco_2020_04_01\nWHERE subject_race IN ('asian/pacific islander','black','hispanic','white' ,'other')\nGROUP BY subject_race\n\nORDER BY city, subject_race;\n\n\nHere I use a side-by-side bar chart to compare stop counts by race across San Diego and San Francisco. Each bar represents the number of stops for a race–city combination.\n\n\nShow code\n#| fig-alt: \"Bar chart comparing stop counts by race in San Diego and San Francisco on April 1, 2020.\"\nlibrary(ggplot2)\n\nggplot(race_counts_2cities,\n       aes(x = subject_race, y = n_stops, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Stops by Race: San Diego vs San Francisco\",\n    x = \"Race\",\n    y = \"Number of Stops\",\n    fill = \"City\"\n  ) \n\n\n\n\n\n\n\n\n\nThis visualization depicts the difference in total stops between the two cities. There are a few obvious limitations, first the lack of proportionality limits the explanatory or correlative power of the visualization. However, some conclusions can be drawn about the cities. First, San Francisco consistently records a higher number of stops across all racial categories, reflecting its larger data set and larger population. In both cities, White and Asian/Pacific Islander individuals make up the largest volume of stops. Hispanic and Black individuals represent fewer total stops in both cities, though the relative ordering of racial groups remains similar between the two jurisdictions, with a curious parity between stops for Hispanics in San Diego and San Francisco. The broad similarity in ranking across cities suggests comparable demographic or enforcement patterns, although the absolute differences highlight differing levels of police activity or population size.\n\n\nVehicular vs. Pedestrian Stops by Race in Oakland\nAfter comparing stop patterns across two cities, the second section focuses on differences within cities. Oakland is one of the few cities in the data set that distinguishes stop types into 2 categories, such as vehicular and pedestrian, allowing us to examine whether different racial groups are disproportionately represented in certain kinds of stops. This provides a more detailed look at how policing practices may vary across stop types, complementing the broader cross-city comparison in Section 1.\nThe SQL query below pulls counts of stops in Oakland, broken down by subject race and stop type (vehicular vs pedestrian), again using the same five racial categories.\n\n\nShow code\n#Count Oakland stops by race and stop type (vehicular + pedestrian)\nSELECT\n  subject_race,\n  `type`, -- vehicular or pedestrians\n  COUNT(*) AS n_stops\nFROM ca_oakland_2020_04_01\nWHERE subject_race IN ('asian/pacific islander', 'black', 'hispanic', 'other', 'white')\n  AND type IN ('vehicular', 'pedestrian')\nGROUP BY subject_race, type\nORDER BY subject_race, type;\n\n\nHere I create a bar chart showing, for each race, the number of vehicular and pedestrian stops. This helps highlight whether the composition of stops by race changes when we move from one stop type to another.\n\n\nShow code\n#| fig-alt: \"Side-by-side bar chart showing the number of vehicular and pedestrian stops by race in Oakland\n\nlibrary(ggplot2)\n\nggplot(oak_stops_by_type,\n       aes(x = subject_race, y = n_stops, fill = type)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Vehicular vs Pedestrian Stops by Race in Oakland\",\n    x = \"Race\",\n    y = \"Number of Stops\",\n    fill = \"Stop Type\"\n  ) +\n  theme(axis.text.x = element_text(angle = 25, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe first clear trend in Oakland’s visualization is that vehicular stops overwhelmingly dominate pedestrian stops across all racial groups. This suggests that most of the police–public contact reported in the Stanford study occured through vehicle enforcement rather than interactions with pedestrians. Furthermore, Black residents experience the highest number of vehicular stops by a substantial margin, far exceeding all other racial groups. This stands out in contrast to the San Diego and San Francisco patterns observed before, where white and Asian/Pacific Islander groups accounted for the largest raw stop numbers. Pedestrian stops, while much smaller in volume, mirror the same general racial ordering as vehicular stops, with Black individuals again experiencing the highest number.\n\n\nConclusion\nIn San Diego and San Francisco, stop counts by race show similar ordering but different overall volumes. In Oakland, stop type matters, since vehicular stops dominate, and Black residents appear most often in both vehicular and pedestrian stops.\nBecause the underlying data were constructed and studied in the Stanford Open Policing Project (Pierson et al., 2020), they also serve as an example of the kinds of administrative records that can be used to evaluate racial disparities or to train more complex statistical models. As with the AFST and predictive policing cases, the data show who appears most often in the record, not necessarily who poses the greatest risk.\n\n\nShow code\ndbDisconnect(con_traffic)\n\n\nReferences\nPierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Jenson, D., Shoemaker, A., Ramachandran, V., et al. (2020). A large-scale analysis of racial disparities in police stops across the United States. Nature Human Behaviour, 4, 736–745.\nStanford Open Policing Project. (2020). Traffic stop data for California jurisdictions [Dataset]. https://openpolicing.stanford.edu"
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "Variance in Syrian Refugee Recognition Across Europe (2014-2024)",
    "section": "",
    "text": "This project uses UNHCR asylum decision data (2014–2024) to investigate whether country-level recognition rates for Syrian asylum seekers differ systematically between two blocs:\n1) the “EU Big-4” comprised of Germany (DEU), France (FRA), Italy (ITA), Spain (ESP)\n2: Non-EU High-Income Nations: Norway (NOR), United Kingdom (GBR), Switzerland (CHE), Iceland (ISL)\nThese countries, while still being European, have key differences in their asylum administrative structure, legal frameworks governing refugee protection, processing volume and bureaucratic capacity, political systems overseeing refugee policy. For this study, bloc membership is assued to represent distinct asylum-policy regimes.\nThus, the research question is: Do EU Big-4 countries exhibit higher recognition rates for Syrian asylum applicants than Non-EU high-income European nations?"
  },
  {
    "objectID": "Project_3.html#hypotheses",
    "href": "Project_3.html#hypotheses",
    "title": "Variance in Syrian Refugee Recognition Across Europe (2014-2024)",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nNull Hypothesis (H₀):\nThere is no systematic difference in recognition rates between the EU Big-4 and the Non-EU high-income bloc. Bloc membership is unrelated to recognition-rate behavior.\n\n\nAlternative Hypothesis (H₁):\nThe EU nations have higher recognition rates, on average and in median, than Non-EU European high-income Nations.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(refugees)\nview(asylum_decisions)\n\n\nThis code creates a data frame that keeps the relevant EU and Nordic countries that received asylum claims from Syria between the years of 2014 and 2024. It selects the COO ISO code for Syria (“SYR”), the COA ISO codes for the EU and Non - EU respectively, then calculates their recognition rates using rec_total, which summarizes the decision(whether it was through official recognition channels through UNHCR’s mandate or other forms of asylum protection, and n_total, which is the total number of applications received by a given country, and dividing rec_total by n_total. It then outputs a tibble with each asylum country, their rate, and their bloc. It also removes NAs and prevents dividing by 0 during the rate calculations. It also returns an table with the mean and median recognition rates aggregated by bloc.\n\n\nShow code\n# Focus on Syrian origin cases\norigin_sel &lt;- \"SYR\"\n\nref_slice &lt;- asylum_decisions |&gt;\n  # keep only the variables we need\n  \n  select(year, coo_iso, coa_iso, dec_recognized, dec_other, dec_total) |&gt;\n  \n  # filters for Syrian origin, between 2014–2024, and the receiving countries of interest. \n  filter(\n    coo_iso == origin_sel,\n    year &gt;= 2014, year &lt;= 2024,\n    (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\" |\n     coa_iso == \"NOR\" | coa_iso == \"GBR\" | coa_iso == \"CHE\" | coa_iso == \"ISL\")\n  ) |&gt;\n  group_by(coa_iso) |&gt;\n  summarise(\n    \n    # total recognitions = refugee + other protection\n    rec_total = sum(dec_recognized + dec_other, na.rm = TRUE),\n    \n    # total decisions\n    n_total   = sum(dec_total, na.rm = TRUE),\n    \n    # if no decisions, rec_rate is NA\n    rec_rate  = if_else(n_total &gt; 0, rec_total / n_total, NA_real_),\n    .groups   = \"drop\"\n  ) |&gt;\n  mutate(\n    # assign each country to a bloc using the new definition\n    bloc = case_when(\n      (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\") ~ \"EU Big-4\",\n      (coa_iso == \"NOR\" | coa_iso == \"GBR\" | coa_iso == \"CHE\" | coa_iso == \"ISL\") ~ \"Non-EU High-Income\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  # keep only rows with a valid bloc and recognition rate\n  filter(!is.na(bloc), !is.na(rec_rate)) |&gt;\n  # set bloc as an ordered factor for plotting\n  mutate(bloc = factor(bloc, levels = c(\"EU Big-4\", \"Non-EU High-Income\")))\n\nref_slice\n\n\n# A tibble: 8 × 5\n  coa_iso rec_total n_total rec_rate bloc              \n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;             \n1 CHE         20959   24875    0.843 Non-EU High-Income\n2 DEU        918795 1147929    0.800 EU Big-4          \n3 ESP         19225   22570    0.852 EU Big-4          \n4 FRA         36636   43479    0.843 EU Big-4          \n5 GBR         21066   23888    0.882 Non-EU High-Income\n6 ISL           290     605    0.479 Non-EU High-Income\n7 ITA          4382    5442    0.805 EU Big-4          \n8 NOR         16831   21407    0.786 Non-EU High-Income\n\n\nThe below code provides the aggregate median and average recognition rates between the EU and Non-EU countries.\n\n\nShow code\nbloc_avgs_unweighted &lt;- ref_slice |&gt;\n  group_by(bloc) |&gt;\n  summarise(\n    mean_rate   = mean(rec_rate),\n    median_rate = median(rec_rate),\n    n_countries = n(),\n    .groups = \"drop\"\n  )\n\nbloc_avgs_unweighted\n\n\n# A tibble: 2 × 4\n  bloc               mean_rate median_rate n_countries\n  &lt;fct&gt;                  &lt;dbl&gt;       &lt;dbl&gt;       &lt;int&gt;\n1 EU Big-4               0.825       0.824           4\n2 Non-EU High-Income     0.748       0.814           4\n\n\nAggregating by bloc, the EU Big-4 have a slightly higher mean recognition rate (0.83 vs 0.75), while the medians are very close (0.824 for the EU Big-4 and 0.814 for the Non-EU High-Income group). The lower mean for the Non-EU group is driven in part by Iceland’s much lower recognition rate (~0.48), while Switzerland, the UK, and Norway are clustered in the same general range as the EU Big-4 countries. So even before doing a formal test, the table suggests only a modest bloc difference.\n\nThis boxplot compares country-level recognition rates for Syrian applicants across the EU Big-4 and the Non-EU High-Income bloc, with each point representing one country’s pooled rate from 2014–2024.\n\n\nShow code\nggplot(ref_slice, aes(x = bloc, y = rec_rate)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(\n    title = \"Country recognition rates for SYR refugees (2014–2024)\",\n    x = NULL, y = \"Recognition rate\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nThe EU Big-4 are tightly clustered, all around 0.80–0.85, which produces a narrow box and short whiskers. The Non-EU High-Income group shows much more spread: Switzerland and the UK sit near the top of the distribution, Norway is slightly lower, and Iceland stands out as a clear low-recognition outlier. As a result, the Non-EU box is taller and its whiskers extend further down. The medians of the two blocs are relatively similar, and the boxes overlap substantially, which suggests that any bloc-level gap is small relative to the differences between individual countries within each group.\n\nThe permutation function below takes the observed recognition rates, randomly reassigns them to the two blocs under the null hypothesis of no bloc effect, and computes the mean and median differences for each permuted dataset. Mapping this function over many iterations (here, 5,000) produces the null distributions we compare against the observed gap. This allows us to evaluate how extreme the real-world difference is relative to what would occur by chance alone.\n\n\nShow code\n  perm_data &lt;- function(rep, data){\n  data |&gt; \n    select(bloc, rec_rate) |&gt; \n    mutate(rec_rate_perm = sample(rec_rate, replace = FALSE)) |&gt; \n    group_by(bloc) |&gt; \n    summarise(obs_ave = mean(rec_rate),      \n              obs_med = median(rec_rate),\n              perm_ave = mean(rec_rate_perm),\n              perm_med = median(rec_rate_perm)) |&gt;\n    summarise(obs_ave_diff = diff(obs_ave),\n              obs_med_diff = diff(obs_med),\n              perm_ave_diff = diff(perm_ave),\n              perm_med_diff = diff(perm_med),\n              rep = rep)\n  }\n\n\n\n\nShow code\nset.seed(47)\n\nperm_stats &lt;- map(c(1:5000), perm_data, data = ref_slice) |&gt; \n  list_rbind()\n\n\n\n\nShow code\nperm_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe permutation distribution for the mean difference exhibits several peaks, again reflecting the discrete nature of the statistic when only eight observations are available. The red line marks the observed mean difference between blocs (EU Big-4 minus Non-EU High-Income). The red line lies inside the main body of the null distribution, which indicates that the observed difference in mean recognition rates is not extreme relative to what we would expect if the recognition rates were randomly assigned to either bloc.\n\n\nShow code\nperm_stats |&gt; \n  ggplot(aes(x = perm_med_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_med_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe permutation distribution for the median difference displays three distinct clusters. This occurs because the dataset contains only eight countries, so there are only a limited number of possible median differences when randomly redistributing these eight recognition rates into two groups of four. The red vertical line marks the observed median difference from the real data, which is slightly negative. Its position falls near the center cluster of the permutation distribution rather than in one of the tails. This means that the observed median gap is entirely consistent with what would be expected purely by chance if bloc membership had no true effect on recognition rates. In other words, the observed median difference is not unusually large or small in the context of the null model. This visual inspection suggests that we should fail to reject the null hypothesis for the median.\n\nAcross 2014–2024, the permutation test produces a mean-difference p-value of 0.708 and a median-difference p-value of 0.726. These values indicate that over 70% of the permuted datasets produced bloc differences as large as or larger than the one observed in the real data.\n\n\nShow code\nperm_stats |&gt; \n  summarize(p_val_ave = mean(perm_ave_diff &gt; obs_ave_diff),\n            p_val_med = mean(perm_med_diff &gt; obs_med_diff))\n\n\n# A tibble: 1 × 2\n  p_val_ave p_val_med\n      &lt;dbl&gt;     &lt;dbl&gt;\n1     0.674     0.712\n\n\nBecause these p-values are so high, the observed differences fall squarely within what we would expect purely by chance if bloc membership (EU Big-4 vs. Non-EU High-Income) had no real effect on recognition rates.\nConclusion:\n\nWe fail to reject the null hypothesis. There is no statistically detectable difference in Syrian asylum recognition rates between the EU Big-4 and the Non-EU High-Income countries. The observed bloc differences are small, unstable, and entirely compatible with random variation arising from having only eight countries in the comparison.\nOverall, this project followed the workflow of a formal permutation-based inference study. I began by defining meaningful policy blocs, computing country-level recognition rates, and visualizing bloc differences. I then constructed a simulation-based null model and generated 5,000 permuted data sets to form the distribution for mean and median differences. Comparing the observed statistics to these null distributions allowed me to draw population conclusions about bloc-level asylum recognition patterns.\nSources:\nUNHCR refugees R package — overview & install guide:\nhttps://www.unhcr.org/refugee-statistics/insights/explainers/refugees-r-package.html\nOriginal Source: NHCR Asylum Decisions Dataset (Original Source).\nUnited Nations High Commissioner for Refugees (UNHCR). Refugee Status Determination and Decisions Data, 2014–2024. Retrieved from https://www.unhcr.org/refugee-statistics/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher Hussey",
    "section": "",
    "text": "Hi! My name’s Christopher (though, I usually go by Chris), and welcome to my website! I’m a Politics, Philosophy, and Economics major at Pomona College, with a burgeoning interest in Data Science and all the visualizations that come with it. Some of my academic areas of interest are security, international relations, Post Structuralist philosophy, and surveillance! Outside of class, you can catch me skating, planning events, and stressing about class!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]