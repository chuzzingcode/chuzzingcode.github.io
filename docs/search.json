[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher Hussey",
    "section": "",
    "text": "Hi! My name’s Christopher (though, I usually go by Chris), and welcome to my website! I’m a Politics, Philosophy, and Economics major at Pomona College, with a burgeoning interest in Data Science and all the visualizations that come with it. Some of my academic areas of interest are security, international relations, Post Structuralist philosophy, and surveillance! Outside of class, you can catch me skating, planning events, and stressing about class!"
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "Variance in Syrian Refugee Recognition Across Europe (2014-2024)",
    "section": "",
    "text": "In this project I test whether country-level recognition rates for Syrian asylum applicants differ between two European blocs: the EU Big-4 (Germany, France, Italy, Spain) and the Nordics (Sweden, Norway, Denmark). I focus on the Syrian refugees, ranging from the 2014-17 refugee crisis, to recent regional conflict, culminating in using data from 2014–2024. I compute each country’s recognition rate compare bloc means/medians, and assess significance with a permutation test.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(refugees)\n\n\nThis code creates a data frame that keeps the relevant EU and Nordic countries that received asylum claims from Syria between the years of 2014 and 2024. It selects the COO ISO code for Syria (“SYR”), the COA ISO codes for the EU and Nordic Blocs respectively, then calculates their recognition rates using rec_total, which summarizes the decision(whether it was through official recognition channels through UNHCR’s mandate or other forms of asylum protection, and n_total, which is the total number of applications received by a given country, and dividing rec_total by n_total. It then outputs a tibble with each asylum country, their rate, and their bloc. It also removes NAs and prevents dividing by 0 during the rate calculations. It also returns an table with the mean and median recognition rates aggregated by bloc.\n\n\nShow code\n# focus\norigin_sel &lt;- \"SYR\"\n\nref_slice &lt;- asylum_decisions|&gt;\n  select(year, coo_iso, coa_iso, dec_recognized, dec_other, dec_total)|&gt;\n  filter(coo_iso == origin_sel,\n         year &gt;= 2014, year &lt;= 2024,\n         (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\" |\n          coa_iso == \"SWE\" | coa_iso == \"NOR\" | coa_iso == \"DNK\"))|&gt;\n  group_by(coa_iso)|&gt;\n  summarise(\n    rec_total = sum(dec_recognized + dec_other, na.rm = TRUE),\n    n_total   = sum(dec_total, na.rm = TRUE),\n    rec_rate  = if_else(n_total &gt; 0, rec_total / n_total, NA_real_),\n  )|&gt;\n    mutate(\n    bloc = case_when(\n      (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\") ~ \"EU Big-4\",\n      (coa_iso == \"SWE\" | coa_iso == \"NOR\" | coa_iso == \"DNK\")                   ~ \"Nordics\",\n      TRUE ~ NA_character_\n    )\n  )|&gt;\n  filter(!is.na(bloc), !is.na(rec_rate))|&gt;\n  mutate(bloc = factor(bloc, levels = c(\"EU Big-4\",\"Nordics\")))\n\nref_slice\n\n\n# A tibble: 7 × 5\n  coa_iso rec_total n_total rec_rate bloc    \n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   \n1 DEU        918795 1147929    0.800 EU Big-4\n2 DNK         18343   19321    0.949 Nordics \n3 ESP         19225   22570    0.852 EU Big-4\n4 FRA         36636   43479    0.843 EU Big-4\n5 ITA          4382    5442    0.805 EU Big-4\n6 NOR         16831   21407    0.786 Nordics \n7 SWE         95822  117367    0.816 Nordics \n\n\nThe below code provides the aggregate median and average recognition rates between EU and Nordic countries.\n\n\nShow code\nbloc_avgs_unweighted &lt;- ref_slice |&gt;\n  group_by(bloc) |&gt;\n  summarise(\n    mean_rate   = mean(rec_rate),\n    median_rate = median(rec_rate),\n    n_countries = n(),\n    .groups = \"drop\"\n  )\n\nbloc_avgs_unweighted\n\n\n# A tibble: 2 × 4\n  bloc     mean_rate median_rate n_countries\n  &lt;fct&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;int&gt;\n1 EU Big-4     0.825       0.824           4\n2 Nordics      0.851       0.816           3\n\n\nThis plot compares country-level recognition rates for Syrian applicants across the EU Big-4 and the Nordics. The central lines (medians) are close, and the boxes/whiskers overlap substantially. Within the Nordics there’s visibly more dispersion, but the EU Big-4 are tighter with lower-variance. Visually, any bloc gap looks small relative to country-to-country variation.\n\n\nShow code\nggplot(ref_slice, aes(x = bloc, y = rec_rate)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(\n    title = \"Country recognition rates for SYR refugees (2014–2024)\",\n    x = NULL, y = \"Recognition rate\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nBased on this data, My research question would be, do the Nordic countries on average and on median have a lower recognition rate than the EU big 4 for Syrian Refugees?\nMy Null hypothesis would be there is no difference in the average or median recognition rate between Nordic and Big 4 EU countries.\nMy alternative hypothesis is that EU big 4 countries have a higher median and average recognition rate than Nordic countries.\nI used the below function to randomly assign data points to each country in the study. The null hypothesis would assume that the random assignment should produce test statistics similar to the observed statistics!\n\n\nShow code\n  perm_data &lt;- function(rep, data){\n  data |&gt; \n    select(bloc, rec_rate) |&gt; \n    mutate(rec_rate_perm = sample(rec_rate, replace = FALSE)) |&gt; \n    group_by(bloc) |&gt; \n    summarise(obs_ave = mean(rec_rate),      \n              obs_med = median(rec_rate),\n              perm_ave = mean(rec_rate_perm),\n              perm_med = median(rec_rate_perm)) |&gt;\n    summarise(obs_ave_diff = diff(obs_ave),\n              obs_med_diff = diff(obs_med),\n              perm_ave_diff = diff(perm_ave),\n              perm_med_diff = diff(perm_med),\n              rep = rep)\n  }\n\n\nI will generate 500 simulated null data sets to show the distribution of permuted mean and median differences on a histogram. A red line will show the observed mean and median differences in the original data set.\n\n\nShow code\nset.seed(47)\n\nperm_stats &lt;- map(c(1:5000), perm_data, data = ref_slice) |&gt; \n  list_rbind()\n\n\nperm_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nShow code\nperm_stats |&gt; \n  ggplot(aes(x = perm_med_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_med_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nDue to the red line’s proximity to the center of the distribution in both, it seems the data has failed to reject the null hypothesis. This means it is likely there is not a significant difference between the observed values and the expected difference To verify there is no real difference, the below data will provide the P-value.\n\n\nShow code\nperm_stats |&gt; \n  summarize(p_val_ave = mean(perm_ave_diff &gt; obs_ave_diff),\n            p_val_med = mean(perm_med_diff &gt; obs_med_diff))\n\n\n# A tibble: 1 × 2\n  p_val_ave p_val_med\n      &lt;dbl&gt;     &lt;dbl&gt;\n1     0.350     0.436\n\n\nAcross 2014–2024, we fail to reject the null hypothesis. the EU Big-4 and Nordics do not exhibit a statistically detectable difference in country-level recognition rates for Syrian applicants when comparing bloc means/medians. The P-value signifies the likelihood that an observed result is due to random chance. Below, we see a P value for the average recognition rate of 34%, and for the median it is 43%. This means that there is a respective chance of 34% or 43% that the observed result is due to a random occurrence, and thus not enough to reject the null hypothesis\nSources:\nUNHCR refugees R package — overview & install guide:\n\nhttps://www.unhcr.org/refugee-statistics/insights/explainers/refugees-r-package.html"
  },
  {
    "objectID": "Project_5.html",
    "href": "Project_5.html",
    "title": "Project_5",
    "section": "",
    "text": "Show code\nlibrary(DBI)\nlibrary(RMariaDB)\n\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)"
  },
  {
    "objectID": "Project_5.html#task",
    "href": "Project_5.html#task",
    "title": "Project_5",
    "section": "Task",
    "text": "Task\nYour task is to use at least 3 data tables to say something interesting about traffic stops (or pedestrian stops). It will take a little bit of sleuthing to figure out what you want to look at. In the end, you will present your findings in a figure or table. Some potential areas to explore include:\n\nAre there differences over time?\nAre there differences across datasets?\nIs there anything interesting about pedestrian stops versus vehicular stops? (I’m not sure how many of the datasets have much information on pedestrian stops).\n\nYour analysis should have the following elements:\n\nqueries of at least three of the SQL tables\nat least 6 uses of SQL keywords other than SELECT, FROM, and LIMIT.\nat least 2 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables\na reference / documentation of the data source (see below, make sure to include the citation!)1"
  },
  {
    "objectID": "Project_4.html",
    "href": "Project_4.html",
    "title": "Weaponized Data in the AFST",
    "section": "",
    "text": "The Allegheny Family Screening Tool and the Ethics of Predictive Welfare Surveillance\nWhat Is the AFST and Why It Matters\nThe Allegheny Family Screening Tool (AFST) is an algorithm used by Allegheny County, Pennsylvania to help child welfare workers decide whether to investigate a hotline call about child neglect. When a report is filed, the system automatically pulls information from multiple public databases—welfare benefits, criminal justice involvement, juvenile probation, mental health records, and public school data—and uses that information to generate a risk score from 1 to 20, meant to predict the likelihood that a child will be removed from the home within two years.\nAccording to the Pulitzer Center (Ho & Burke, 2022), the AFST was developed to help caseworkers manage high call volumes, reduce human bias, and allocate agency resources efficiently. The system is often described as a neutral, objective supplemental tool designed to assist human decision-making. However, as Ballantyne (2023) argues, the algorithm operates in a high-stakes context—its predictions can shape whether families experience intrusive investigations, heightened state surveillance, or even the possibility of temporary or permanent child removal. This combination of administrative data, predictive modeling, and discretionary state power makes the AFST a powerful example of the data science ethical dilemmas highlighted in Data Feminism, including the central questions: Who benefits? Who is harmed? and Whose priorities are embedded in the system?\nIn what follows, I examine four core ethical issues—consent, representativeness, unintended data use, and racial proxies—and analyze how each plays out within this human-algorithm collaboration between social workers and algorithmic predictions.\n#1 Consent Structure\nThe AFST raises significant concerns about whether meaningful consent is possible. Families whose information populates the system never knowingly agreed to have their welfare, education, or criminal justice records combined into an algorithmic decision-making tool. The Pulitzer Center article emphasizes that most families do not even know the algorithm exists, much less that it influences whether a caseworker opens an investigation into their home (Ho & Burke, 2022). This means that, despite the deeply personal nature of the data involved, individuals cannot control how their information is used, cannot opt out, and cannot reasonably understand how ordinary interactions with public systems—such as applying for food stamps or receiving school attendance letters—may later be treated as risk indicators.\nBallantyne (2023) further notes that consent is complicated by the inherent power imbalance between families and the state agencies that collect their data. Families interacting with welfare, education, or criminal justice systems typically have no choice but to provide information as a condition of receiving services or complying with legal requirements. This is not voluntary participation in research—it is coerced disclosure embedded in structural inequality. Therefore, informed consent is not only absent; the very possibility of informed consent is undermined by the conditions under which the data are collected.\n#2: Who Was Measured / Representativeness\nA core issue is who appears in the dataset. The AFST trains exclusively on data from families who interact with public systems, meaning poor and disproportionately Black families are the ones most heavily monitored. The Pulitzer Center makes clear that the algorithm draws from welfare records, school data, public mental health services, and criminal justice involvement—systems that are not evenly distributed across socioeconomic lines (Ho & Burke, 2022). Families with higher income who use private healthcare, private schools, or do not rely on public benefits leave far fewer digital traces and therefore remain largely invisible to the model.\nThis creates a severe representativeness problem. The algorithm purports to assess risk for children across the county, but in practice, it can only “see” those who are already hyper-surveilled through state systems. As Ballantyne (2023) points out, the model does not measure underlying risk; it measures exposure to public institutions. This means the AFST may conflate poverty with danger, treating structural disadvantage as evidence of impending neglect. From the perspective of Data Feminism, this demonstrates how power operates—the groups already monitored by the state become the objects of even greater scrutiny.\n#3: Is the Data Being Used in Unintended Ways?\nThe AFST’s use of administrative data illustrates a classic case of function creep, where information collected for one purpose is repurposed for a different, often more punitive, goal. Welfare records were originally gathered to distribute benefits, school records to track educational progress, and criminal justice data to document legal processes. But according to the Pulitzer Center, this disparate information is now merged to generate family risk scores (Ho & Burke, 2022). The algorithm reframes routine or structural conditions—such as unemployment, food insecurity, or living in a heavily policed neighborhood—as potential indicators of future child maltreatment.\nBallantyne (2023) argues that this repurposing fundamentally changes the meaning of the data: information that once signaled a need for support now becomes evidence the state could act upon. This raises ethical concerns about whether individuals should be subject to predictive analysis based on data they provided for unrelated administrative reasons. The recontextualization of data, without public awareness or consent, expands the reach of state surveillance under the guise of efficiency.\n#4: Should Race Be Used? Is It a Proxy?\nEven though race is not explicitly included as a variable, the AFST incorporates numerous proxy variables that reflect racial disparities. Variables such as zip code, welfare usage, school suspensions, juvenile probation, and arrest records are deeply correlated with race due to long-standing structural inequities. The Pulitzer Center article notes that these patterns cause families of color—especially Black families—to receive disproportionately higher risk scores (Ho & Burke, 2022). Removing the race variable does not remove racial influence; it only hides it behind factors shaped by residential segregation, policing practices, and unequal access to resources.\nBallantyne (2023) points out that the algorithm’s design reinforces existing inequities by reproducing the racial patterns embedded in the data. Instead of addressing the conditions that shape the data (poverty, discrimination, over-policing), the tool treats these structural forces as individual-level risk factors. This mirrors a key question from the Data Feminism framework: Is the issue really “bias in the algorithm,” or is the algorithm simply reflecting unjust systems that already exist?\nWhy This Matters:\nThe AFST illustrates how data science systems can shift power toward institutions and away from the people they govern. Who benefits? The county and its caseworkers gain efficiency and predictive tools that help allocate scarce resources. Who is harmed? Families who are already marginalized face heightened surveillance, invasive investigations, and greater risk of family separation. As Ballantyne (2023) emphasizes, even the best-intentioned human–AI collaborations can cause harm when they reinforce unequal power structures. And as the Pulitzer Center shows, the ethical dilemmas here are not hypothetical—the algorithm’s risk scores shape real decisions with life-altering consequences.\nUltimately, this case reflects the broader pattern described in Data Feminism: data science is often used “in the service of surveillance (of the minoritized) and efficiency (amidst scarcity).” The AFST is not just a technical tool; it is a form of governance that makes certain families more visible to the state while others remain unseen. It embeds social inequalities into mathematical form, presenting them as objective truth. This matters because data-driven decisions often carry an aura of neutrality that masks the power relations beneath. Understanding these dynamics is crucial if data science is to be used ethically and responsibly in social systems.\n7. References\nBallantyne, N. (2023). The harm that data do: The case of the Allegheny Family Screening Tool. Medium. https://medium.com/@neilballantyne/the-harm-that-data-do-the-case-of-the-allegheny-family-screening-tool-5f9fca22e0b2\nHo, S., & Burke, G. (2022). An algorithm that screens for child neglect raises concerns. Pulitzer Center. https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns"
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Patterns in Netflix Shows",
    "section": "",
    "text": "This page looks at the TidyTuesday Netflix Titles dataset from 2021-04-20, which includes metadata and short descriptions for movies and TV shows. In my plots, I compare movie runtimes by decade and title lengths across shows in the data set.\n\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\n\n\n\nnetflix &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv\",\n)\n\nclean_data &lt;- netflix |&gt;\n  select(type, title, date_added, release_year, duration, description) |&gt;\n  mutate(\n    title       = str_squish(title),\n    description = str_squish(description),\n    decade      = (release_year %/% 10) * 10\n  )\n\nparsed_data &lt;- clean_data |&gt;\n  mutate(\n    minutes = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*min$)\")),\n    seasons = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*Season(?:s)?$)\"))\n  )\n\n\n\n\nShow code\nparsed_data |&gt;\n  filter(type == \"Movie\", !is.na(minutes), !is.na(decade)) |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  ggplot(aes(x = minutes, fill = decade)) +\n  geom_density(alpha = 0.25) +\n  labs(\n    title = \"Movie runtime distributions by decade\",\n    x = \"Runtime (minutes)\",\n    y = \"Density\",\n    fill = \"Decade\"\n  )\n\n\n\n\n\n\n\n\n\nEach curve on this plot shows the distribution of Netflix movie runtimes for each decade represented since the 40s. The higher peaks show the most common lengths for that decade. What we see in this plot are the changes across time for what a typical runtime looks like in each era, with the peaks showing what the most frequent lengths were for a given decade.\n\n\nShow code\nparsed_data |&gt;\n  transmute(\n    title_words = title |&gt;\n      str_squish() |&gt;\n      str_count(\"\\\\S+\")         \n  ) |&gt;\n  filter(!is.na(title_words)) |&gt;\n  ggplot(aes(x = title_words)) +\n  geom_histogram(binwidth = 1, center = 0.5) +\n  labs(\n    title = \"How long are Netflix titles?\",\n    subtitle = \"Histogram of the number of words per title\",\n    x = \"Words in title\",\n    y = \"Number of titles\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar on this plot shows how many Netflix titles have a given number of words in their names. Taller bars mark the most common title lengths. What we see is that short, punchy names (around 1–3 words) are most frequent, with a tapering tail of longer titles that tend to appear less often.\n\n\nShow the code\nparsed_data |&gt;\n  filter(type == \"TV Show\", !is.na(seasons), seasons &lt;= 11) |&gt;\n  count(seasons, sort = FALSE) |&gt;\n  mutate(seasons = factor(seasons, levels = sort(unique(seasons)))) |&gt;\n  ggplot(aes(x = seasons, y = n)) +\n  geom_col() +\n  labs(\n    title = \"How many seasons do Netflix TV shows have?\",\n    subtitle = \"Counts of TV shows by stated season count (shows with &gt;11 seasons excluded)\",\n    x = \"Seasons\",\n    y = \"Number of TV shows\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar shows how many Netflix TV shows report a given number of seasons, 1 - 11. The tallest bars at the low season counts show that most Netflix series in the catalog are shorter/only have 1 season. This plot excluded series with more than 11 seasons for readability.\nReferences:\n\nTidyTuesday – Netflix Titles (2021-04-20)\nrfordatascience/tidytuesday (CSV + README).\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-04-20\nOriginal Dataset – Netflix Movies and TV Shows (Kaggle)\nCommunity-compiled catalog commonly cited as the underlying source.\nhttps://www.kaggle.com/datasets/shivamb/netflix-shows"
  },
  {
    "objectID": "tidydata2.html",
    "href": "tidydata2.html",
    "title": "My Second Tidy Visualization",
    "section": "",
    "text": "# Clean data provided by Kat Correia. Student interns extracted this information\n# from reproductive medicine journals using duplicate data entry. \n# After duplicate data entry discrepancies were resolved, their \n# individual files were combined, and posted for public access \n# as Google sheets, e.g.,:\n# https://docs.google.com/spreadsheets/d/1UuWGQxRU0wxVuOZGYnvkwKn-GdmLIj8kzmUm4KLRHQY/edit?gid=1719021104#gid=1719021104\n# No additional cleaning was necessary.\n\narticle_dat &lt;- readr::read_csv(\"https://kcorreia.people.amherst.edu/repro_med_disparities-article-level-data.csv\")\n\nRows: 318 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (26): doi, jabbrv, journal, month, day, title, abstract, keywords, study...\ndbl (35): pmid, year, study_year_start, study_year_end, race1_ss, race2_ss, ...\nlgl  (4): eth7, eth7_ss, eth8, eth8_ss\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmodel_dat &lt;- readr::read_csv(\"https://kcorreia.people.amherst.edu/repro_med_disparities-model-level-data.csv\")\n\nRows: 6804 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): doi, stratified, stratgrp, subanalysis, subgrp, outcome, measure, ...\ndbl  (5): model_number, comparison, point, lower, upper\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "tidydata1.html",
    "href": "tidydata1.html",
    "title": "Lack of Complete Indoor Plumbing: 2023 vs 2022",
    "section": "",
    "text": "In this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nTidy Tuesday Data\nCensus Data: U.S. Census Bureau. American Community Survey (ACS) 1-year estimates, Tables B01003 & B25049 (2022–2023)."
  },
  {
    "objectID": "tidydata1.html#this-data-set-came-from-the-u.s.-census-bureaus-american-community-survey-acs-for-2022-and-2023.-it-tracks-county-level-population-and-the-share-of-households-without-complete-plumbing-letting-us-see-how-access-to-basic-facilities-has-changed-over-time.",
    "href": "tidydata1.html#this-data-set-came-from-the-u.s.-census-bureaus-american-community-survey-acs-for-2022-and-2023.-it-tracks-county-level-population-and-the-share-of-households-without-complete-plumbing-letting-us-see-how-access-to-basic-facilities-has-changed-over-time.",
    "title": "Lack of Complete Indoor Plumbing: 2023 vs 2022",
    "section": "",
    "text": "In this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nTidy Tuesday Data\nCensus Data: U.S. Census Bureau. American Community Survey (ACS) 1-year estimates, Tables B01003 & B25049 (2022–2023)."
  }
]