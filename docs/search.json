[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher Hussey",
    "section": "",
    "text": "Hi! My name’s Christopher (though, I usually go by Chris), and welcome to my website! I’m a Politics, Philosophy, and Economics major at Pomona College, with a burgeoning interest in Data Science and all the visualizations that come with it. Some of my academic areas of interest are security, international relations, Post Structuralist philosophy, and surveillance! Outside of class, you can catch me skating, planning events, and stressing about class!"
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "Variance in Syrian Refugee Recognition Across Europe (2014-2024)",
    "section": "",
    "text": "In this project I test whether country-level recognition rates for Syrian asylum applicants differ between two European blocs: the EU Big-4 (Germany, France, Italy, Spain) and the Nordics (Sweden, Norway, Denmark). I focus on the Syrian refugees, ranging from the 2014-17 refugee crisis, to recent regional conflict, culminating in using data from 2014–2024. I compute each country’s recognition rate compare bloc means/medians, and assess significance with a permutation test.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(refugees)\n\n\nThis code creates a data frame that keeps the relevant EU and Nordic countries that received asylum claims from Syria between the years of 2014 and 2024. It selects the COO ISO code for Syria (“SYR”), the COA ISO codes for the EU and Nordic Blocs respectively, then calculates their recognition rates using rec_total, which summarizes the decision(whether it was through official recognition channels through UNHCR’s mandate or other forms of asylum protection, and n_total, which is the total number of applications received by a given country, and dividing rec_total by n_total. It then outputs a tibble with each asylum country, their rate, and their bloc. It also removes NAs and prevents dividing by 0 during the rate calculations. It also returns an table with the mean and median recognition rates aggregated by bloc.\n\n\nShow code\n# focus\norigin_sel &lt;- \"SYR\"\n\nref_slice &lt;- asylum_decisions|&gt;\n  select(year, coo_iso, coa_iso, dec_recognized, dec_other, dec_total)|&gt;\n  filter(coo_iso == origin_sel,\n         year &gt;= 2014, year &lt;= 2024,\n         (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\" |\n          coa_iso == \"SWE\" | coa_iso == \"NOR\" | coa_iso == \"DNK\"))|&gt;\n  group_by(coa_iso)|&gt;\n  summarise(\n    rec_total = sum(dec_recognized + dec_other, na.rm = TRUE),\n    n_total   = sum(dec_total, na.rm = TRUE),\n    rec_rate  = if_else(n_total &gt; 0, rec_total / n_total, NA_real_),\n  )|&gt;\n    mutate(\n    bloc = case_when(\n      (coa_iso == \"DEU\" | coa_iso == \"FRA\" | coa_iso == \"ITA\" | coa_iso == \"ESP\") ~ \"EU Big-4\",\n      (coa_iso == \"SWE\" | coa_iso == \"NOR\" | coa_iso == \"DNK\")                   ~ \"Nordics\",\n      TRUE ~ NA_character_\n    )\n  )|&gt;\n  filter(!is.na(bloc), !is.na(rec_rate))|&gt;\n  mutate(bloc = factor(bloc, levels = c(\"EU Big-4\",\"Nordics\")))\n\nref_slice\n\n\n# A tibble: 7 × 5\n  coa_iso rec_total n_total rec_rate bloc    \n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   \n1 DEU        918795 1147929    0.800 EU Big-4\n2 DNK         18343   19321    0.949 Nordics \n3 ESP         19225   22570    0.852 EU Big-4\n4 FRA         36636   43479    0.843 EU Big-4\n5 ITA          4382    5442    0.805 EU Big-4\n6 NOR         16831   21407    0.786 Nordics \n7 SWE         95822  117367    0.816 Nordics \n\n\nThe below code provides the aggregate median and average recognition rates between EU and Nordic countries.\n\n\nShow code\nbloc_avgs_unweighted &lt;- ref_slice |&gt;\n  group_by(bloc) |&gt;\n  summarise(\n    mean_rate   = mean(rec_rate),\n    median_rate = median(rec_rate),\n    n_countries = n(),\n    .groups = \"drop\"\n  )\n\nbloc_avgs_unweighted\n\n\n# A tibble: 2 × 4\n  bloc     mean_rate median_rate n_countries\n  &lt;fct&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;int&gt;\n1 EU Big-4     0.825       0.824           4\n2 Nordics      0.851       0.816           3\n\n\nThis plot compares country-level recognition rates for Syrian applicants across the EU Big-4 and the Nordics. The central lines (medians) are close, and the boxes/whiskers overlap substantially. Within the Nordics there’s visibly more dispersion, but the EU Big-4 are tighter with lower-variance. Visually, any bloc gap looks small relative to country-to-country variation.\n\n\nShow code\nggplot(ref_slice, aes(x = bloc, y = rec_rate)) +\n  geom_boxplot() +\n  geom_jitter()+\n  labs(\n    title = \"Country recognition rates for SYR refugees (2014–2024)\",\n    x = NULL, y = \"Recognition rate\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nBased on this data, My research question would be, do the Nordic countries on average and on median have a lower recognition rate than the EU big 4 for Syrian Refugees?\nMy Null hypothesis would be there is no difference in the average or median recognition rate between Nordic and Big 4 EU countries.\nMy alternative hypothesis is that EU big 4 countries have a higher median and average recognition rate than Nordic countries.\nI used the below function to randomly assign data points to each country in the study. The null hypothesis would assume that the random assignment should produce test statistics similar to the observed statistics!\n\n\nShow code\n  perm_data &lt;- function(rep, data){\n  data |&gt; \n    select(bloc, rec_rate) |&gt; \n    mutate(rec_rate_perm = sample(rec_rate, replace = FALSE)) |&gt; \n    group_by(bloc) |&gt; \n    summarise(obs_ave = mean(rec_rate),      \n              obs_med = median(rec_rate),\n              perm_ave = mean(rec_rate_perm),\n              perm_med = median(rec_rate_perm)) |&gt;\n    summarise(obs_ave_diff = diff(obs_ave),\n              obs_med_diff = diff(obs_med),\n              perm_ave_diff = diff(perm_ave),\n              perm_med_diff = diff(perm_med),\n              rep = rep)\n  }\n\n\nI will generate 500 simulated null data sets to show the distribution of permuted mean and median differences on a histogram. A red line will show the observed mean and median differences in the original data set.\n\n\nShow code\nset.seed(47)\n\nperm_stats &lt;- map(c(1:5000), perm_data, data = ref_slice) |&gt; \n  list_rbind()\n\n\n# --- null histograms with observed vertical lines ---\nperm_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nShow code\nperm_stats |&gt; \n  ggplot(aes(x = perm_med_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_med_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\nDue to the red line’s proximity to the center of the distribution in both, it seems the data has failed to reject the null hypothesis. This means it is likely there is not a significant difference between the observed values and the expected difference To verify there is no real difference, the below data will provide the P-value.\n\n\nShow code\nperm_stats |&gt; \n  summarize(p_val_ave = mean(perm_ave_diff &gt; obs_ave_diff),\n            p_val_med = mean(perm_med_diff &gt; obs_med_diff))\n\n\n# A tibble: 1 × 2\n  p_val_ave p_val_med\n      &lt;dbl&gt;     &lt;dbl&gt;\n1     0.350     0.436\n\n\nAcross 2014–2024, we fail to reject the null hypothesis. the EU Big-4 and Nordics do not exhibit a statistically detectable difference in country-level recognition rates for Syrian applicants when comparing bloc means/medians. The P-value signifies the likelihood that an observed result is due to random chance. Below, we see a P value for the average recognition rate of 34%, and for the median it is 43%. This means that there is a respective chance of 34% or 43% that the observed result is due to a random occurrence, and thus not enough to reject the null hypothesis\nSources:\nUNHCR refugees R package — overview & install guide:\n\nhttps://www.unhcr.org/refugee-statistics/insights/explainers/refugees-r-package.html"
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Patterns in Netflix Shows",
    "section": "",
    "text": "This page looks at the TidyTuesday Netflix Titles dataset from 2021-04-20, which includes metadata and short descriptions for movies and TV shows. In my plots, I compare movie runtimes by decade and title lengths across shows in the data set.\n\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\n\n\n\nnetflix &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv\",\n)\n\nclean_data &lt;- netflix |&gt;\n  select(type, title, date_added, release_year, duration, description) |&gt;\n  mutate(\n    title       = str_squish(title),\n    description = str_squish(description),\n    decade      = (release_year %/% 10) * 10\n  )\n\nparsed_data &lt;- clean_data |&gt;\n  mutate(\n    minutes = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*min$)\")),\n    seasons = as.integer(str_extract(duration, \"(?&lt;=^)\\\\d+(?=\\\\s*Season(?:s)?$)\"))\n  )\n\n\n\n\nShow code\nparsed_data |&gt;\n  filter(type == \"Movie\", !is.na(minutes), !is.na(decade)) |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  ggplot(aes(x = minutes, fill = decade)) +\n  geom_density(alpha = 0.25) +\n  labs(\n    title = \"Movie runtime distributions by decade\",\n    x = \"Runtime (minutes)\",\n    y = \"Density\",\n    fill = \"Decade\"\n  )\n\n\n\n\n\n\n\n\n\nEach curve on this plot shows the distribution of Netflix movie runtimes for each decade represented since the 40s. The higher peaks show the most common lengths for that decade. What we see in this plot are the changes across time for what a typical runtime looks like in each era, with the peaks showing what the most frequent lengths were for a given decade.\n\n\nShow code\nparsed_data |&gt;\n  transmute(\n    title_words = title |&gt;\n      str_squish() |&gt;\n      str_count(\"\\\\S+\")         \n  ) |&gt;\n  filter(!is.na(title_words)) |&gt;\n  ggplot(aes(x = title_words)) +\n  geom_histogram(binwidth = 1, center = 0.5) +\n  labs(\n    title = \"How long are Netflix titles?\",\n    subtitle = \"Histogram of the number of words per title\",\n    x = \"Words in title\",\n    y = \"Number of titles\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar on this plot shows how many Netflix titles have a given number of words in their names. Taller bars mark the most common title lengths. What we see is that short, punchy names (around 1–3 words) are most frequent, with a tapering tail of longer titles that tend to appear less often.\n\n\nShow the code\nparsed_data |&gt;\n  filter(type == \"TV Show\", !is.na(seasons), seasons &lt;= 11) |&gt;\n  count(seasons, sort = FALSE) |&gt;\n  mutate(seasons = factor(seasons, levels = sort(unique(seasons)))) |&gt;\n  ggplot(aes(x = seasons, y = n)) +\n  geom_col() +\n  labs(\n    title = \"How many seasons do Netflix TV shows have?\",\n    subtitle = \"Counts of TV shows by stated season count (shows with &gt;11 seasons excluded)\",\n    x = \"Seasons\",\n    y = \"Number of TV shows\"\n  )\n\n\n\n\n\n\n\n\n\nEach bar shows how many Netflix TV shows report a given number of seasons, 1 - 11. The tallest bars at the low season counts show that most Netflix series in the catalog are shorter/only have 1 season. This plot excluded series with more than 11 seasons for readability.\nReferences:\n\nTidyTuesday – Netflix Titles (2021-04-20)\nrfordatascience/tidytuesday (CSV + README).\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-04-20\nOriginal Dataset – Netflix Movies and TV Shows (Kaggle)\nCommunity-compiled catalog commonly cited as the underlying source.\nhttps://www.kaggle.com/datasets/shivamb/netflix-shows"
  },
  {
    "objectID": "tidydata2.html",
    "href": "tidydata2.html",
    "title": "My Second Tidy Visualization",
    "section": "",
    "text": "# Clean data provided by Kat Correia. Student interns extracted this information\n# from reproductive medicine journals using duplicate data entry. \n# After duplicate data entry discrepancies were resolved, their \n# individual files were combined, and posted for public access \n# as Google sheets, e.g.,:\n# https://docs.google.com/spreadsheets/d/1UuWGQxRU0wxVuOZGYnvkwKn-GdmLIj8kzmUm4KLRHQY/edit?gid=1719021104#gid=1719021104\n# No additional cleaning was necessary.\n\narticle_dat &lt;- readr::read_csv(\"https://kcorreia.people.amherst.edu/repro_med_disparities-article-level-data.csv\")\n\nRows: 318 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (26): doi, jabbrv, journal, month, day, title, abstract, keywords, study...\ndbl (35): pmid, year, study_year_start, study_year_end, race1_ss, race2_ss, ...\nlgl  (4): eth7, eth7_ss, eth8, eth8_ss\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmodel_dat &lt;- readr::read_csv(\"https://kcorreia.people.amherst.edu/repro_med_disparities-model-level-data.csv\")\n\nRows: 6804 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): doi, stratified, stratgrp, subanalysis, subgrp, outcome, measure, ...\ndbl  (5): model_number, comparison, point, lower, upper\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "tidydata1.html",
    "href": "tidydata1.html",
    "title": "Lack of Complete Indoor Plumbing: 2023 vs 2022",
    "section": "",
    "text": "In this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nTidy Tuesday Data\nCensus Data: U.S. Census Bureau. American Community Survey (ACS) 1-year estimates, Tables B01003 & B25049 (2022–2023)."
  },
  {
    "objectID": "tidydata1.html#this-data-set-came-from-the-u.s.-census-bureaus-american-community-survey-acs-for-2022-and-2023.-it-tracks-county-level-population-and-the-share-of-households-without-complete-plumbing-letting-us-see-how-access-to-basic-facilities-has-changed-over-time.",
    "href": "tidydata1.html#this-data-set-came-from-the-u.s.-census-bureaus-american-community-survey-acs-for-2022-and-2023.-it-tracks-county-level-population-and-the-share-of-households-without-complete-plumbing-letting-us-see-how-access-to-basic-facilities-has-changed-over-time.",
    "title": "Lack of Complete Indoor Plumbing: 2023 vs 2022",
    "section": "",
    "text": "In this visualization, each point represents an overlapping county included in both 2022 and 2023 versions of American Community Survey with a ≥65k population. If plumbing access remained the same all points would fall along the diagonal trend line. The scatter shows that while many counties cluster close to that line, some saw higher shares of households lacking complete plumbing in 2023 (points above the line), and others saw improvements (points below).\nTidy Tuesday Data\nCensus Data: U.S. Census Bureau. American Community Survey (ACS) 1-year estimates, Tables B01003 & B25049 (2022–2023)."
  }
]